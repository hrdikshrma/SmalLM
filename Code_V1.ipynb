{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a81b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc8a84",
   "metadata": {},
   "source": [
    "# Step 1 : Data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4812c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 57004\n"
     ]
    }
   ],
   "source": [
    "with open(\"mega.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "print(\"Total number of character:\", len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f706f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecular Biology and Evolution, 2024, 41, 1–9\n",
      "https://doi.org/10.1093/molbev/msae263\n",
      "Advance access publication 21 December 2024\n",
      "Resources\n",
      "                                                                                                                                       MBE\n",
      "MEGA12: Molecular Evolutionary Genetic Analysis Version\n",
      "12 for Adaptive and Green Computing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                               Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Sudhir Kumar ,1,2,* Glen Stecher,1 Michael Suleski,1 Maxwell Sanderford,1 Sudip Sharma                                                                 ,1,2\n",
      "Koichiro Tamura 3,4\n",
      "1\n",
      "  Institute for Genomics and Evolutionary Medicine, Temple University, Philadelphia, PA 19122, USA\n",
      "2\n",
      "  Department of Biology, Temple University, Philadelphia, PA 19122, USA\n",
      "3\n",
      "  Department of Biological Sciences, Tokyo Metropolitan University, Tokyo, Japan\n",
      "4\n",
      "  Research Center for Genomics and Bioinformatics, Tokyo Metropolitan University, Tokyo, Japan\n",
      "*Corresponding author: E-mail: s.kumar@temple.edu.\n",
      "Associate editor: Fabia Ursula Battistuzzi\n",
      "\n",
      "\n",
      "Abstract\n",
      "We introduce the 12th version of the Molecular Evolutionary Genetics Analysis (MEGA12) software. This latest version brings many significant\n",
      "improvements by reducing the computational time needed for selecting optimal substitution models and conducting bootstrap tests on\n",
      "phylogenies using maximum likelihood (ML) methods. These improvements are achieved by implementing heuristics that minimize likely\n",
      "unnecessary computations. Analyses of empirical and simulated datasets show substantial time savings by using these heuristics without\n",
      "compromising the accuracy of results. MEGA12 also links-in an evolutionary sparse learning approach to identify fragile clades and associated\n",
      "sequences in evolutionary trees inferred through phylogenomic analyses. In addition, this version includes fine-grained parallelization for ML\n",
      "analyses, support for high-resolution monitors, and an enhanced Tree Explorer. MEGA12 can be downloaded from https://www.megasoftware.net.\n",
      "Keywords: software, phylogenomics, model selection, bootstrap, green computing.\n",
      "\n",
      "\n",
      "Introduction                                                                    sequence alignments (MSAs) were generated with varying\n",
      "The Molecular Evolutionary Genetics Analysis (MEGA) soft­                       sequence lengths, number of sequences, base frequencies,\n",
      "ware is extensively used for molecular evolution and phyloge­                   substitution rates, heterogeneity across sites, and proportion\n",
      "netics (Kumar 2022). It offers many computational tools,                        of invariant sites. A total of 24 models of substitutions (six\n",
      "including maximum likelihood (ML), maximum parsimony                            base models and their +I, +G, and +I+G combinations; see\n",
      "(MP), ordinary least squares, Bayesian, and distance-based                      main text) were used for simulating the data, with 300 data­\n",
      "methods (Fig. 1). Some popular functionalities in MEGA are                      sets generated for each model scenario. From each model\n",
      "the selection of optimal nucleotide and amino acid substitution                 category, we randomly selected 10 datasets (out of 300), re­\n",
      "models, inference of evolutionary relationships, tests of phylog­               sulting in a total of 240 simulated datasets for model selec­\n",
      "enies using the bootstrap method, estimation of sequence diver­                 tion analysis. Sequence counts in these datasets ranged\n",
      "gences and times, and the reconstruction of ancestral sequences                 from 4 to 289, which were 186 to 18,171 sites long. We per­\n",
      "(supplementary fig. S1, Supplementary Material online).                         formed bootstrapping with the Adaptive option and model\n",
      "   Notably, phylogenetic ML analyses are time-consuming and                     selection with the Filtered option. Then, we estimated their\n",
      "contribute to a substantial carbon footprint (Kumar 2022). To                   concordance by comparing results from standard bootstrap\n",
      "tackle these challenges, the latest update of MEGA has aimed to                 and model selection with the Full option, respectively.\n",
      "improve the computational efficiency of ML analyses. This has                      We also analyzed amino acid sequence datasets, generated\n",
      "been achieved by developing and implementing heuristic ap­                      from a concatenated MSA (Chloroplast_Martin.meg). The\n",
      "proaches that avoid unnecessary calculations while preserving ac­               dataset is an example dataset in MEGA12 (Adachi et al.\n",
      "curacy. Further efficiencies are gained by optimizing parallel                  2000; Tamura et al. 2021). Forty-five MSAs were generated\n",
      "computations to make the best use of available computing resour­                using the protein domain boundaries from the concatenated\n",
      "ces. In the following sections, we will discuss these updates and en­           alignment. We performed the model selection analyses for\n",
      "hancements to the graphical user interface (GUI), including                     these protein domains and compared the results from the\n",
      "seamless access to an external application (DrPhylo) for detecting              use of the Full and Filtered options.\n",
      "fragile clades and associated sequences in the inferred phylogenies.               The DrPhylo analysis was conducted on a clade within a fun­\n",
      "                                                                                gus phylogeny derived from an ML analysis of an empirical data­\n",
      "                                                                                set comprising 1,233 protein-coding nuclear genes from 86 yeast\n",
      "Materials and Methods\n",
      "                                                                                species (Shen et al. 2017). This clade was selected because it was\n",
      "Datasets Analyzed                                                               reported to be fragile in previous research studies (Shen et al.\n",
      "Simulated datasets were obtained from a previously published                    2017; Sharma and Kumar 2024). The ML tree and MSAs for\n",
      "research study (Abadi et al. 2019). These nucleotide multiple                   each protein-coding gene were obtained from Shen et al. (2017).\n",
      "\n",
      "Received: October 2, 2024. Revised: December 12, 2024. Accepted: December 17, 2024. Published online: December 21, 2024\n",
      "© The Author(s) 2024. Published by Oxford University Press on behalf of Society for Molecular Biology and Evolution.\n",
      "This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\n",
      "permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\n",
      "\f",
      "2                                                                                          Kumar et al. · https://doi.org/10.1093/molbev/msae263\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                          Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Fig. 1. Main GUI of MEGA12. a) The main toolbar provides access to various analytical capabilities organized in drop-down menus. b) One of the\n",
      "drop-down menus is shown on the main window. c) The AppTile provides access to the linked DrPhylo application, which is also accessible from the Tree\n",
      "Explorer window (see Fig. 4a). d) The OutputTile provides access to results from DrPhylo analysis via a drop-down menu (e). f) Clicking the Prototype\n",
      "button allows for the building of a “.mao” analysis configuration file for the command-line analysis using MEGA-CC. It is necessary to click Analyze to\n",
      "return to the standard mode to conduct analysis using the GUI.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Options for Analyses Conducted                                                 models describe the instantaneous probabilities of nucleotide\n",
      "We used MEGA12 for all analyses to directly compare the im­                    substitutions at individual sites. They can be combined with\n",
      "pact of certain new features while keeping all other aspects the               a (discretized) gamma distribution of rate variation among\n",
      "same because many incremental changes and bug fixes have                       sites (indicated by +G) and the presence/absence of invariant\n",
      "been made over the 3 years since MEGA11 was released.                          sites (indicated by +I), which are reviewed in Nei and Kumar\n",
      "For the model selection analysis, we first selected the best-fit               (2000).\n",
      "substitution models using the Full option, which tested all                       The computational time required for the ML analysis of\n",
      "the substitution models. The best-fit models found in these                    24 different model combinations increases with data size\n",
      "analyses were used as ground truth for model selection results                 (Fig. 2a). Here, data size is quantified by multiplying the\n",
      "obtained using the Filtered option. Default BIC and AICc                       number of sequences (S) by the distinct number of site config­\n",
      "thresholds of 5 were used in these analyses. For the bootstrap                 urations (C) in the MSA. Distinct site configurations are used\n",
      "analysis, the Standard option in MEGA12 was used with 500                      because all sites with the same configuration, patterns of\n",
      "replicates, and the Adaptive option was used with a default                    bases present across sequences, are compressed into a single\n",
      "standard error (SE) threshold of 5%.                                           column with \n"
     ]
    }
   ],
   "source": [
    "print(text_data[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f942da",
   "metadata": {},
   "source": [
    "# Step 2 : Tokenise the given input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1635a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### A naive way is to tokenize the data is to either break on the spaces but that would result in not very efficient tokenization for example if we have 2 words like \"boy\" and  \"boys\", they will be treated as 2 separate tokens. Even if they are close in meaning, they will be represented as 2 separate tokens. Also, it is difficult to handle OOV (Out-of-Vocabulary) text using the above technique and the dictionary can be as big as it can get. This is called Word based tokenization\n",
    "\n",
    "#### Another way can be to break the text into characters. This solves the OOV problem as there are definte characters in English language. But, the semantic meaning and ordering is completely lost. Also, the tokenized sequence is much longer than the original text. This is called Character based tokenization\n",
    "\n",
    "#### Therefore, all the big LLM companies use Sub-word based tokenization techniques. For example Byte Pair Encoding. The frequent pair of characters will appear together. More information on the link : https://github.com/openai/tiktoken\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c599254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed75a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3271c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ddba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17353, 345, 588, 428, 2900, 656, 257, 10273, 1410, 284, 1061, 625, 362, 1906, 18, 1933, 30]\n",
      "Would you like this turned into a weekly plan to follow over 2–3 months?\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "text = (\"Would you like this turned into a weekly plan to follow over 2–3 months?\")\n",
    "token_IDs = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_IDs)\n",
    "\n",
    "string = tokenizer.decode(token_IDs)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44652267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLY THE TIKTOKEN TOKENIZER ON OUR DATA\n",
    "data_tokens = tokenizer.encode(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802007ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecular Biology and Evolution, 2024, 41, 1–9\n",
      "https://doi.org/10.1093/molbev/msae263\n",
      "Advance access publication 21 December 2024\n",
      "Resources\n",
      "                                                                                                                                       MBE\n",
      "MEGA12: Molecular Evolutionary Genetic Analysis Version\n",
      "12 for Adaptive and Green Computing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                               Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Sudhir Kumar ,1,2,* Glen Stecher,1 Michael Suleski,1 Maxwell Sanderford,1 Sudip Sharma                                                                 ,1,2\n",
      "Koichiro Tamura 3,4\n",
      "1\n",
      "  Institute for Genomics and Evolutionary Medicine, Temple University, Philadelphia, PA 19122, USA\n",
      "2\n",
      "  Department of Biology, Temple University, Philadelphia, PA 19122, USA\n",
      "3\n",
      "  Department of Biological Sciences, Tokyo Metropolitan University, Tokyo, Japan\n",
      "4\n",
      "  Research Center for Genomics and Bioinformatics, Tokyo Metropolitan University, Tokyo, Japan\n",
      "*Corresponding author: E-mail: s.kumar@temple.edu.\n",
      "Associate editor: Fabia Ursula Battistuzzi\n",
      "\n",
      "\n",
      "Abstract\n",
      "We introduce the 12th version of the Molecular Evolutionary Genetics Analysis (MEGA12) software. This latest version brings many significant\n",
      "improvements by reducing the computational time needed for selecting optimal substitution models and conducting bootstrap tests on\n",
      "phylogenies using maximum likelihood (ML) methods. These improvements are achieved by implementing heuristics that minimize likely\n",
      "unnecessary computations. Analyses of empirical and simulated datasets show substantial time savings by using these heuristics without\n",
      "compromising the accuracy of results. MEGA12 also links-in an evolutionary sparse learning approach to identify fragile clades and associated\n",
      "sequences in evolutionary trees inferred through phylogenomic analyses. In addition, this version includes fine-grained parallelization for ML\n",
      "analyses, support for high-resolution monitors, and an enhanced Tree Explorer. MEGA12 can be downloaded from https://www.megasoftware.net.\n",
      "Keywords: software, phylogenomics, model selection, bootstrap, green computing.\n",
      "\n",
      "\n",
      "Introduction                                                                    sequence alignments (MSAs) were generated with varying\n",
      "The Molecular Evolutionary Genetics Analysis (MEGA) soft­                       sequence lengths, number of sequences, base frequencies,\n",
      "ware is extensively used for molecular evolution and phyloge­                   substitution rates, heterogeneity across sites, and proportion\n",
      "netics (Kumar 2022). It offers many computational tools,                        of invariant sites. A total of 24 models of substitutions (six\n",
      "including maximum likelihood (ML), maximum parsimony                            base models and their +I, +G, and +I+G combinations; see\n",
      "(MP), ordinary least squares, Bayesian, and distance-based                      main text) were used for simulating the data, with 300 data­\n",
      "methods (Fig. 1). Some popular functionalities in MEGA are                      sets generated for each model scenario. From each model\n",
      "the selection of optimal nucleotide and amino acid substitution                 category, we randomly selected 10 datasets (out of 300), re­\n",
      "models, inference of evolutionary relationships, tests of phylog­               sulting in a total of 240 simulated datasets for model selec­\n",
      "enies using the bootstrap method, estimation of sequence diver­                 tion analysis. Sequence counts in these datasets ranged\n",
      "gences and times, and the reconstruction of ancestral sequences                 from 4 to 289, which were 186 to 18,171 sites long. We per­\n",
      "(supplementary fig. S1, Supplementary Material online).                         formed bootstrapping with the Adaptive option and model\n",
      "   Notably, phylogenetic ML analyses are time-consuming and                     selection with the Filtered option. Then, we estimated their\n",
      "contribute to a substantial carbon footprint (Kumar 2022). To                   concordance by comparing results from standard bootstrap\n",
      "tackle these challenges, the latest update of MEGA has aimed to                 and model selection with the Full option, respectively.\n",
      "improve the computational efficiency of ML analyses. This has                      We also analyzed amino acid sequence datasets, generated\n",
      "been achieved by developing and implementing heuristic ap­                      from a concatenated MSA (Chloroplast_Martin.meg). The\n",
      "proaches that avoid unnecessary calculations while preserving ac­               dataset is an example dataset in MEGA12 (Adachi et al.\n",
      "curacy. Further efficiencies are gained by optimizing parallel                  2000; Tamura et al. 2021). Forty-five MSAs were generated\n",
      "computations to make the best use of available computing resour­                using the protein domain boundaries from the concatenated\n",
      "ces. In the following sections, we will discuss these updates and en­           alignment. We performed the model selection analyses for\n",
      "hancements to the graphical user interface (GUI), including                     these protein domains and compared the results from the\n",
      "seamless access to an external application (DrPhylo) for detecting              use of the Full and Filtered options.\n",
      "fragile clades and associated sequences in the inferred phylogenies.               The DrPhylo analysis was conducted on a clade within a fun­\n",
      "                                                                                gus phylogeny derived from an ML analysis of an empirical data­\n",
      "                                                                                set comprising 1,233 protein-coding nuclear genes from 86 yeast\n",
      "Materials and Methods\n",
      "                                                                                species (Shen et al. 2017). This clade was selected because it was\n",
      "Datasets Analyzed                                                               reported to be fragile in previous research studies (Shen et al.\n",
      "Simulated datasets were obtained from a previously published                    2017; Sharma and Kumar 2024). The ML tree and MSAs for\n",
      "research study (Abadi et al. 2019). These nucleotide multiple                   each protein-coding gene were obtained from Shen et al. (2017).\n",
      "\n",
      "Received: October 2, 2024. Revised: December 12, 2024. Accepted: December 17, 2024. Published online: December 21, 2024\n",
      "© The Author(s) 2024. Published by Oxford University Press on behalf of Society for Molecular Biology and Evolution.\n",
      "This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\n",
      "permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\n",
      "\f",
      "2                                                                                          Kumar et al. · https://doi.org/10.1093/molbev/msae263\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                          Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Fig. 1. Main GUI of MEGA12. a) The main toolbar provides access to various analytical capabilities organized in drop-down menus. b) One of the\n",
      "drop-down menus is shown on the main window. c) The AppTile provides access to the linked DrPhylo application, which is also accessible from the Tree\n",
      "Explorer window (see Fig. 4a). d) The OutputTile provides access to results from DrPhylo analysis via a drop-down menu (e). f) Clicking the Prototype\n",
      "button allows for the building of a “.mao” analysis configuration file for the command-line analysis using MEGA-CC. It is necessary to click Analyze to\n",
      "return to the standard mode to conduct analysis using the GUI.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Options for Analyses Conducted                                                 models describe the instantaneous probabilities of nucleotide\n",
      "We used MEGA12 for all analyses to directly compare the im­                    substitutions at individual sites. They can be combined with\n",
      "pact of certain new features while keeping all other aspects the               a (discretized) gamma distribution of rate variation among\n",
      "same because many incremental changes and bug fixes have                       sites (indicated by +G) and the presence/absence of invariant\n",
      "been made over the 3 years since MEGA11 was released.                          sites (indicated by +I), which are reviewed in Nei and Kumar\n",
      "For the model selection analysis, we first selected the best-fit               (2000).\n",
      "substitution models using the Full option, which tested all                       The computational time required for the ML analysis of\n",
      "the substitution models. The best-fit models found in these                    24 different model combinations increases with data size\n",
      "analyses were used as ground truth for model selection results                 (Fig. 2a). Here, data size is quantified by multiplying the\n",
      "obtained using the Filtered option. Default BIC and AICc                       number of sequences (S) by the distinct number of site config­\n",
      "thresholds of 5 were used in these analyses. For the bootstrap                 urations (C) in the MSA. Distinct site configurations are used\n",
      "analysis, the Standard option in MEGA12 was used with 500                      because all sites with the same configuration, patterns of\n",
      "replicates, and the Adaptive option was used with a default                    bases present across sequences, are compressed into a single\n",
      "standard error (SE) threshold of 5%.                                           column with a corresponding frequency in the ML analysis\n",
      "   Model selection and bootstrap analyses were conducted us­                   (Sharma and Kumar 2021). Due to the extensive use of model\n",
      "ing the command-line version of MEGA12 (“megacc”) with a                       selection by a large user base of MEGA (supplementary fig.\n",
      "single thread for direct comparisons. DrPhylo analysis was                     S2, Supplementary Material online), these analyses’\n",
      "conducted using MEGA12’s GUI. In all these analyses, we                        combined computational cost—and consequently, energy\n",
      "used a 64-bit desktop computer with 8 logical processors                       consumption—is enormous (Kumar 2022).\n",
      "(3.36 GHz) and 64 GB of system memory running the                                 To accelerate model selection, we have developed a heuristic\n",
      "Windows 10 operating system.                                                   approach that reduces the number of substitution model com­\n",
      "                                                                               binations tested, eliminating potentially suboptimal models\n",
      "                                                                               early in the process. This new heuristic for model selection\n",
      "Results\n",
      "                                                                               analysis of nucleotide sequence alignments begins with evalu­\n",
      "Adaptive Computing in Selecting the Optimal                                    ating the ML model fit for 6 base models: GTR, HKY, TN93,\n",
      "Substitution Model                                                             T92, K2P, and JC. MEGA uses the Bayesian information cri­\n",
      "Selecting the most suitable substitution model is often the ini­               terion (BIC) and the corrected Akaike information criterion\n",
      "tial step in molecular phylogenetics. The ML method for model                  (AICc) to evaluate model fit where these criteria are calculated\n",
      "selection was initially introduced in MEGA5 (Tamura et al.                     based on the log-likelihood fit of each model to the given MSA\n",
      "2011) and has been frequently used (supplementary fig. S1,                     and its associated parameters (Tamura et al. 2011). Therefore,\n",
      "Supplementary Material online). MEGA assesses 6 primary                        MEGA first determines the BIC and AICc for these 6 base\n",
      "nucleotide substitution models to determine the optimal mod­                   models. The base model with the lowest BIC value (BICmin)\n",
      "el: general time reversible (GTR), Hasegawa–Kishino–Yano                       is selected first, and AICcmin is taken as the AICc value of\n",
      "(HKY), Tamura–Nei (TN93), Tamura 3-parameter (T92),                            this same model. Base models with BIC or AICc values not ex­\n",
      "Kimura 2-parameter (K2P), and Jukes–Cantor (JC); see Nei                       ceeding 5 points of BICmin or AICcmin, respectively, are consid­\n",
      "and Kumar (2000) for a review. These primary substitution                      ered potentially optimal models for further consideration.\n",
      "\f",
      "MEGA12 for Adaptive and Green Computing · https://doi.org/10.1093/molbev/msae263                                                                               3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                                   Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Fig. 2. Substitution model selection using MEGA12. a) The relationships between the time required for the standard model selection and the data size:\n",
      "the product of the number of sequences (S) and the number of distinct site configurations (C) in the sequence alignment. The time required for model\n",
      "selection analysis increases linearly with the data size. b) MEGA12’s Analysis Preferences dialog box allows users to set options for model selection\n",
      "analysis. The newly added Filtered option is shown, which offers a setting of BIC and AICc thresholds. As the main text explains, a smaller number will\n",
      "result in testing fewer models. c) Time savings are achieved using the Filtered option with default parameters, which is the greatest for datasets for which\n",
      "the full analysis selects a complex best-fit substitution model. d) The relationship between the number of model parameters and the average percentage\n",
      "of model combinations whose ML evaluation was skipped. e) The relationship of time taken with the Filtered and Full options for model selection for\n",
      "chloroplast amino acid MSAs. The slope of the regression line is 0.13, indicating that the Filtered option greatly speeds model selection.\n",
      "\n",
      "Models with AICc or BIC exceeding 5 points of AICcmin or                         in all 8 primary substitution models and eliminates all models\n",
      "BICmin are considered suboptimal, and other model combina­                       with BIC and AICc values 5 more than BICmin or AICcmin, re­\n",
      "tions derived from these suboptimal base models are not tested                   spectively, as outlined above. In the next step, BIC and AICc\n",
      "further.                                                                         are computed for each of the remaining models combined\n",
      "   The ML analysis of the remaining base models in combin­                       with the +F option, using the empirical frequencies from the\n",
      "ation with +I, +G (with 4 categories), and both +I+G is then                     MSA (Tamura et al. 2011). New model combinations are\n",
      "carried out to calculate different information criteria for the fi­              eliminated if their BIC and AICc exceed BICmin or AICcmin, re­\n",
      "nal selection. The model with the lowest BIC is determined as                    spectively, by 5 or more. In the final step, the ML analysis of\n",
      "the best-fit model of substitutions. MEGA outputs different                      the remaining models in combination with +I, +G, and +G+I\n",
      "information criteria scores, log likelihood (lnL), and other                     is then carried out to generate the final result.\n",
      "model parameters for models tested. In MEGA12, this heuris­                         The use of the Filtered option for 45 chloroplast proteins\n",
      "tic can be used by selecting the newly added Filtered option                     (31 to 1,388 amino acids) from 10 species resulted in extensive\n",
      "(Fig. 2b). Users can also choose to set a desired threshold for                  (∼87%) savings compared with the Full analysis (see Fig. 2e).\n",
      "BIC and AICc, with smaller threshold values resulting in the                     The same best-fit substitution models were found as the Full\n",
      "elimination of more models earlier.                                              option for 43 proteins (95% concordance). A statistically in­\n",
      "   In an analysis of 240 simulated datasets generated with substi­               distinguishable model (ΔBIC < 10) was chosen for one of the\n",
      "tution models of various complexities in an independent study                    other 2 datasets, while the second-best model was selected\n",
      "(Abadi et al. 2019) (see Materials and Methods), the Filtered op­                for the other. Model selection with the Filtered option for con­\n",
      "tion identified the same optimal substitution model as the Full                  catenating 45 chloroplast protein MSAs (11,039 amino acids)\n",
      "analysis for 240 datasets (100% concordance). It achieved as                     took only 3.7 min instead of 27.1 min for the Full analysis\n",
      "much as 70% reduction in computational time when a complex                       while producing the same substitution model.\n",
      "substitution model fits the best (see Fig. 2c). The savings were                    These results suggest that the speedup for model selection\n",
      "lower for simpler models because simpler models are nested                       analysis will be realized for every dataset analyzed with the fil­\n",
      "within more complex models, so the latter may not be eliminated                  tered option in MEGA12. However, the degree of computa­\n",
      "early on. For this reason, the number of model combinations                      tional efficiency gained depends on the complexity of the\n",
      "analyzed is directly related to the number of parameters in the                  substitution model that best fits the data.\n",
      "best-fit model (see Fig. 2d). We anticipate that most users will\n",
      "experience substantial speedups by utilizing the Filtered option\n",
      "because complex models are often the optimal fit for bigger                      Adaptive Bootstrapping\n",
      "datasets.                                                                        After selecting the optimal substitution model, the next step in\n",
      "   The Filtered option is also implemented for amino acid                        phylogenetic analysis is to infer evolutionary relationships and\n",
      "MSAs. In this case, MEGA12 first determines BIC and AICc                         assess the confidence in the monophyly of inferred clades.\n",
      "\f",
      "4                                                                                            Kumar et al. · https://doi.org/10.1093/molbev/msae263\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                              Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Fig. 3. Adaptive bootstrap analysis of the Drosophila Adh dataset. a) MEGA12’s Analysis Preferences dialog box with the new Adaptive option for\n",
      "automatically determining the number of bootstrap replicates. The use of the Adaptive option stops generating bootstrap replicates when the SE of every\n",
      "BS value in the phylogeny becomes <5% (default option). The Threshold option allows setting an alternative SE value for more or less precise BS values.\n",
      "b) MEGA’s updated Tree Explorer with an option to display the range of BS values (±1 SE) for every inferred clade. The results are based on the analysis of\n",
      "the data in the Drosophia_Adh.meg file distributed in the Examples folder with MEGA12.\n",
      "\n",
      "\n",
      "\n",
      "The bootstrap approach has been available in MEGA since                          available for phylogenies inferred using distance-based (e.g.\n",
      "version 1 to estimate confidence in the inferred relationships                   neighbor-joining [NJ]) and MP methods.\n",
      "(Felsenstein 1985; Kumar et al. 1994). In the bootstrap pro­                        MEGA12’s Adaptive option first generates 25 resampled\n",
      "cedure, many resampled MSA are generated by sampling sites                       MSAs. If any of the BS values in the phylogeny has an SE greater\n",
      "with replacement until the number of sites in the resampled                      than or equal to 5%, then additional resampled MSAs are gen­\n",
      "MSA is the same as in the original MSA. Phylogenies are in­                      erated until all the BS values in the phylogeny have achieved an\n",
      "ferred from these resampled MSAs using a phylogenetic tree                       SE < 5%, a threshold that can be set by the user (Fig. 3a). Our\n",
      "estimation approach (e.g. ML approach). The proportion of                        rationale for picking a 5% default was that some of the clades\n",
      "times a cluster of sequences appears in the phylogenies ob­                      in the inferred phylogeny would have BS close to 50%, so\n",
      "tained from the resampled MSAs is its bootstrap support                          achieving                                                     an\n",
      "(BS). A high BS value indicates that the inferred clade is statis­               SE < 5% for these clades will require many replicates (often\n",
      "tically supported (Felsenstein 1985).                                            ∼100). With 100 replicates, the clades with high BS values\n",
      "   Users frequently choose to generate a large number of re­                     will have SE closer to 2.5%. This is evident from the analysis\n",
      "sampled MSAs (500 to 2,000 replicates) because every BS val­                     of Drosophia_Adh.meg dataset distributed in the Examples\n",
      "ue is an estimate whose accuracy is determined by the number                     folder with MEGA12 (Fig. 3b). MEGA12 stopped the boot­\n",
      "of resampled MSAs analyzed (Hedges 1992; Pattengale et al.                       strap procedure after 87 replicates, which resulted in much nar­\n",
      "2010). The number of bootstrap replicates increases the time                     rower ranges (BS–SE to BS + SE) for high BS values than low BS\n",
      "required proportionally, which becomes particularly onerous                      values (Fig. 3b).\n",
      "for computationally intensive ML phylogenetics. To reduce                           We also analyzed all 240 simulated datasets with standard\n",
      "this burden, MEGA12 introduces an Adaptive option for                            and adaptive bootstrap approaches. Figure 4a shows the rela­\n",
      "bootstrap analysis (Fig. 3a), automatically determining the op­                  tionship of the BS values obtained using 500 replicates (x axis)\n",
      "timal number of replicates for the bootstrap analysis. It is                     and determined adaptively (y axis) for 240 datasets. The rela­\n",
      "based on the fact that high BS values, which are of primary                      tionship is strong (R2 = 0.99) with a slope close to 1 (Fig. 4a),\n",
      "interest to researchers, can be estimated with high precision                    i.e. the two approaches produced very similar results overall.\n",
      "(i.e. low SE) from a small number of replicates. For example,                    However, there is some dispersion because we used a thresh­\n",
      "the estimation of BS = 95% with an SE = 2.5% requires only                       old of SE = 5% for adaptive bootstrapping. The number of\n",
      "75 bootstrap replicates. Interestingly, BS values close to                       replicates needed for adaptive bootstrapping varied between\n",
      "50%, often of limited biological interest, require hundreds                      25 and 124 for the 240 datasets, with 25 because of a hard\n",
      "of replicates (390) to reach an SE of 2.5%. Therefore, using                     lower limit placed by MEGA12. Because the default SE thresh­\n",
      "a large number of replicates primarily increases the precision                   old of 5% is applied to every clade in the inferred phylogeny,\n",
      "of BS values close to 50%. To emphasize that the BS values                       the number of bootstrap replicates needed will be determined\n",
      "are estimated with SEs, we have updated Tree Explorer in                         by the node whose estimated BS support is closest to 50% be­\n",
      "MEGA12 to display the range of BS values (±1 SE, Fig. 3b).                       cause the variance of a BS value (say b) is given by b(1 − b)/r,\n",
      "Users have the option to display the BS value or the range cal­                  where r is the number of replicates (Hedges 1992). Indeed,\n",
      "culated using the above formula. This display option is also                     there is a direct relationship between the number of replicates\n",
      "\f",
      "MEGA12 for Adaptive and Green Computing · https://doi.org/10.1093/molbev/msae263                                                                                 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                                     Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Fig. 4. Adaptive bootstrap analysis in MEGA12. a) Comparison of BS values obtained using the Adaptive determination of the number of bootstrap\n",
      "replicates (y axis) and those obtained using 500 bootstrap replicates (x axis). Results from all 240 datasets were pooled together. The slope of the linear\n",
      "regression through the origin is 0.99 (R2 = 0.99). b) The relationship between the minimum |BS − 50%| in phylogeny and the number of replicates needed\n",
      "by the Adaptive analysis. The negative trend (correlation = -0.96) confirms the inverse relationship expected theoretically. c) The relationship of time taken\n",
      "between the Adaptive and Standard Bootstrap approach for estimating statistical support for clade relationships inferred for simulated DNA sequence\n",
      "alignments. The slope of the regression line is ∼0.20, indicating that the Adaptive approach speeds up the BS estimation significantly.\n",
      "\n",
      "\n",
      "\n",
      "needed in adaptive bootstrapping and the minimum |BS − 50%|                          We demonstrate the steps of DrPhylo analysis and its outputs\n",
      "value in the inferred phylogeny in the analysis of 240 datasets                   by analyzing a phylogenetic tree of 86 fungal species inferred\n",
      "(Fig. 4b). For phylogenies with uniformly high BS values (bot­                    from the ML analysis of a concatenated sequence alignment\n",
      "tom right in Fig. 4b), the number of replicates needed is much                    of 1,232 genes (Shen et al. 2017). In this phylogeny, the place­\n",
      "smaller than when the phylogeny contains even one clade                           ment of Ascoidea rubescens in the ML phylogeny was reported\n",
      "with BS value close to 50%. In any case, the Adaptive option                      to be contentious, even though it received 100% standard BS\n",
      "resulted in speedups on an average of 81% times (Fig. 4c), ran­                   (Shen et al. 2017; Sharma and Kumar 2024). The clade of inter­\n",
      "ging from 61% to 95% for these datasets. In general, we expect                    est contains A. rubescens and 43 other species, which is easily\n",
      "such speedups to be realized for all datasets analyzed in                         disrupted by excluding just one gene (Shen et al. 2017;\n",
      "MEGA12, with actual computational efficiency depending on                         Sharma and Kumar 2024). In MEGA12, the user selects the\n",
      "the properties of the data and the bootstrap stability of the                     clade by clicking on the stem branch (Fig. 5a). Then, DrPhylo\n",
      "phylogenetic inference.                                                           analysis is launched by clicking on the menu that appears\n",
      "                                                                                  (Fig. 5b) and choosing options to specify the sequence data,\n",
      "                                                                                  data type, and model grid size (Fig. 5c). We selected the dataset\n",
      "Integration of the DrPhylo Application to Assess the                              that was already active in MEGA and chose to display 20 top\n",
      "Fragility of Inferred Clades                                                      genes along with 20 taxa from the clade of interest (Fig. 5c).\n",
      "MEGA currently uses the concatenation supermatrix approach                           MEGA12 automatically prepares the input sources (align­\n",
      "when the input contains multiple genes, domains, or genomic                       ments and the tree file) for a given analysis and copies them to\n",
      "segments. This approach is effective in producing organismal re­                  DrPhylo’s execution environment. DrPhylo is launched in a sep­\n",
      "lationships with high confidence (Gadagkar et al. 2005; Kumar                     arate child process along with command-line arguments. As\n",
      "et al. 2012a; Song et al. 2012; Kapli et al. 2020; Williams et al.                DrPhylo executes, MEGA captures all analysis progress informa­\n",
      "2020; Sharma and Kumar 2021). However, the concatenation                          tion and displays it in a new progress window. When the DrPhylo\n",
      "supermatrix approach may occasionally lead to incorrect or fra­                   process finishes, MEGA loads all of the results files into memory\n",
      "gile inferences with very high BS due to systematic, modeling,                    and adds a DrP OutputTile (Fig. 1d) to the main form to provide\n",
      "and data-specific biases (Gatesy and Springer 2014; Warnow                        access to output files produced by DrPhylo (Fig. 1e). Output files\n",
      "2015; Sharma and Kumar 2024). MEGA12 now makes avail­                             include model grid (M-grid) in both graphical and textual format,\n",
      "able the DrPhylo approach, a sparse learning approach                             position and gene sparsity score, DrPhylo run log, and options for\n",
      "(Kumar and Sharma 2021; Sharma and Kumar 2024) to identify                        saving or erasing results; see Kumar and Sharma (2021) and\n",
      "inferred clades that may be formed due to data-specific biases                    Sharma and Kumar (2024) for more details. Clicking on a\n",
      "(Sharma and Kumar 2024). Users can launch DrPhylo analysis                        menu item in the DrP OutputTile smartly displays the result.\n",
      "for any clade in the phylogeny displayed in the Tree Explorer                     For example, clicking the first menu item displays the graphics\n",
      "(Fig. 5a), as well as directly from the main MEGA window                          file in the integrated web browser with the figure legend (Fig. 5d).\n",
      "(Fig. 1b). One can choose to use partitions (genes or segments)                      In the grid displayed (called the model grid; M-grid), the\n",
      "in the currently active dataset or divide the data into segments                  horizontal axis represents the genes included in the model,\n",
      "of equal length (Fig. 5b). The user may also provide a list of files,             while the vertical axis represents the species along with the\n",
      "each containing a sequence alignment for a data segment, for                      classification probability for their membership within the\n",
      "DrPhylo analysis (Fig. 5c).                                                       clade of interest. The clade probability (CP) is the smallest\n",
      "   DrPhylo is implemented in the MyESL software (Sanderford                       probability of these probabilities because the most unstable\n",
      "et al. 2024) and linked with MEGA12 by upgrading the source                       member dictates the fragility of the clade. Each cell (say i,j)\n",
      "code that has been previously used to link Muscle (Edgar 2004)                    in the M-grid reflects the gene–species concordance (GSC)\n",
      "with MEGA3 (Kumar et al. 2004). MEGA12 extracts the appli­                        score, which quantifies the support a gene (j) provides for a\n",
      "cation binaries and resources to set up the DrPhylo execution en­                 species (i) within the clade of interest. Green cells indicate\n",
      "vironment with the proper files, structure, and permissions when                  the gene’s positive support for species membership within\n",
      "DrPhylo is run in MEGA12 for the first time after installation.                   the clade, whereas red cells display gene–species combinations\n",
      "\f",
      "6                                                                                            Kumar et al. · https://doi.org/10.1093/molbev/msae263\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                              Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "Fig. 5. Conducting DrPhylo analysis via the Tree Explorer in MEGA12. a) Users select the clade of interest by clicking on its ancestral branch or node\n",
      "(highlighted in green) in the Tree Explorer window. b) The context-sensitive menu, which includes the Launch DrPhylo option, is displayed. c) The dialog\n",
      "box to make selections for DrPhylo analysis. d) A graphical representation of the genetic model of the selected clade in a grid format (M-Grid) is shown\n",
      "along with a descriptive caption. This model and other output files are accessible from the DrP OutputTile (see Fig. 1c). e) Caption showing the details of\n",
      "the DrPhylo analyses and a description of the results.\n",
      "\n",
      "\n",
      "\n",
      "that harbor a discordant signal. Based on the GSC score, the                     times for a given phylogeny. These calculations can be time-\n",
      "color intensity corresponds to the strength of the concordance                   consuming for larger datasets, so MEGA12 now implements\n",
      "or discordance. Sharma and Kumar (2024) provided a detailed                      fine-grained parallelization to speed up the estimation of like­\n",
      "explanation of the M-grid for this dataset.                                      lihood values that are calculated independently for different\n",
      "   The M-grid in Fig. 5d shows that the CP for the clade ana­                    sites at a given node in the phylogeny for a given set of branch\n",
      "lyzed is 0.27 due to A. rubescens receiving a low membership                     lengths and substitution pattern parameter values. Our tests\n",
      "probability. One gene, BUSCOfEOG7W9S51, provides the                             showed a sub-linear reduction in computational time needed,\n",
      "strongest support for the placement of A. rubescens (green cells)                achieving slightly <50% efficiency using 4 threads compared\n",
      "in this clade, but many other genes did not support (red) this                   with a single thread. A larger number of threads could offer\n",
      "placement. Interestingly, BUSCOfEOG7W9S51 also strongly                          slightly higher efficiency depending on the number of sequen­\n",
      "supports other species’ inclusion inside the clade. Therefore,                   ces, variations, and other data attributes. These sub-linear ef­\n",
      "BUSCOfEOG7W9S51 is highly influential for this clade and                         ficiency trends are explained by the fact that substantial\n",
      "places A. rubescens inside it. This gene was also found using                    overhead is involved in distributing calculations to different\n",
      "the ML approach by comparing two alternative hypotheses                          threads. In addition, more than half of the nodes in a phyl­\n",
      "for the placement of A. rubescens (Shen et al. 2017).                            ogeny are terminal nodes at which only a few site configura­\n",
      "Removing this gene from the analysis changed the placement                       tions exist (4 or 20, except for ambiguous states for DNA or\n",
      "of A. rubescens with moderate BS (Shen et al. 2017; Sharma                       protein sequences, respectively), which are not amenable to\n",
      "and Kumar 2024). This is because of BUSCOfEOG7TN012,                             significant savings.\n",
      "which shows a very high disagreement with the placement of\n",
      "A. rubescens in this clade (red cell, Fig. 5d) but not others.\n",
      "Such disagreement was confirmed in the gene tree, which posi­                    Generating Initial Trees for Heuristic Searches for ML\n",
      "tioned A. rubescens far outside the clade of interest (Sharma                    Phylogenies\n",
      "and Kumar 2024). In summary, the availability of DrPhylo in                      Options for automatically generating the initial tree for ML\n",
      "MEGA12 will allow users to investigate fragile species relation­                 tree searching have been modified in MEGA12. When the de­\n",
      "ships and causal sequences for any inferred phylogeny.                           fault option is used, MEGA12 first generates two initial candi­\n",
      "                                                                                 date trees: a NJ tree and a MP tree. The NJ tree is based on\n",
      "Other Improvements for Phylogenetic Analysis                                     evolutionary distances computed using a 1-parameter substi­\n",
      "Using ML                                                                         tution model for nucleotides and amino acid MSAs. To find\n",
      "                                                                                 the MP tree candidate, MEGA12 conducts 10 heuristic\n",
      "Fine-grained Parallelization for ML Analysis                                     searches, each starting with a randomly generated tree sub­\n",
      "Many users use MEGA for ML calculations of branch lengths,                       jected to subtree pruning and regrafting (SPR) branch swap­\n",
      "evolutionary parameters, ancestral states, and divergence                        ping; see Nei and Kumar (2000) for a description. The one\n",
      "\f",
      "MEGA12 for Adaptive and Green Computing · https://doi.org/10.1093/molbev/msae263                                                     7\n",
      "\n",
      "\n",
      "with the minimum tree length is chosen among the 10 MP              and they can see branch lengths and double-click to edit\n",
      "trees. Subsequently, the log likelihood is computed for this        them on the spot, which would come in handy when\n",
      "MP tree and the NJ tree using the 1-parameter substitution          Newick trees need to have branch lengths or divergence times\n",
      "model. The tree with superior log likelihood is selected as         for display or further calculations, such as EP analysis. By de­\n",
      "the initial tree for branch swapping to find the ML tree.           fault, the displayed tree now automatically resizes with the\n",
      "                                                                    window. Moving branches via drag-drop now provides visual\n",
      "Elimination of Computational Bottlenecks                            feedback to the user. The taxon name editing text box was up­\n",
      "Testing and benchmarking ML calculations using increasingly         dated to make the behavior consistent with similar GUI ele­\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                          Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "larger datasets revealed bottlenecks in the code that were not      ments in different operating systems.\n",
      "apparent when using small datasets. For instance, the initial­\n",
      "ization step to generate a map of identical site patterns was       Data Explorer Updates\n",
      "previously done in a way that was too slow for big datasets.        Responsiveness of scrolling with large datasets has been im­\n",
      "MEGA12 makes this step orders of magnitude faster using a           proved for the Sequence Alignment Editor (SAE), Sequence\n",
      "fast hash table. We also identified many instances of redun­        Data Explorer (SDE), and Distance Data Explorer (DDE). A\n",
      "dant initializations (e.g. site configuration maps) and calcula­    taxa name search tool and highlighting of all cells correspond­\n",
      "tions, which have been refactored to speed up calculations.         ing to the current search match have been added for the DDE.\n",
      "                                                                    In both the SDE and DDE, the sorting of taxa can now be by\n",
      "Evolutionary Probability Calculation Updates                        name or by distance to the first taxon. The number of base dif­\n",
      "The evolutionary probability (EP) analysis was introduced in        ferences between the first sequence and all the remaining se­\n",
      "MEGA11 (Tamura et al. 2021) for estimating Bayesian neu­            quences are used in SDE. When taxa are grouped, individual\n",
      "tral probabilities of observing alternative alleles in a species    taxa can be selected/unselected based on many different op­\n",
      "contingent on the given species phylogeny and the MSA (Liu          tions: first of each group, by group size, or group inclusion.\n",
      "et al. 2016). The EP analysis in MEGA has been updated so\n",
      "that user-provided times, specified as branch lengths in a          Dealing with High-Resolution Monitors\n",
      "Newick tree, can be used instead of times computed using            The user experience was severely impacted on computer\n",
      "RelTime (Tamura et al. 2012). Users can also select the focal       monitors with ultra-high resolutions when using MEGA11.\n",
      "sequence via the Analysis Preferences dialog box, which was         Standard graphical components (e.g. buttons, icons, and text)\n",
      "previously restricted to the first sequence in the MSA. The re­     are rendered very small on these very high dots per inch (DPI) dis­\n",
      "sults displayed for the EP calculation have been updated, and       plays. Also, MEGA’s custom visual components, such as the tree\n",
      "the evolutionary timespan of the base (Kumar et al. 2012b)          display in TE and text grids in SAE, were variously affected by\n",
      "and focal sequence bases for each site are included in the out­     changes in DPI and resolution settings. The problems were\n",
      "put CSV file.                                                       more than esthetic, causing clickable GUI components to be\n",
      "                                                                    pushed out of view and unusable in some places. Consequently,\n",
      "Improvements in the GUI                                             we needed to redraw hundreds of icons in multiple resolutions\n",
      "The GUI has been updated extensively with many usability im­        and then program MEGA to automatically select the optimal\n",
      "provements and modifications to keep pace with computer             resolution icon images based on the DPI of the monitor.\n",
      "hardware, accessories, and operating system changes.                Furthermore, we have updated all the forms and dialog boxes\n",
      "                                                                    to auto-adjust the size and placement of components based on\n",
      "                                                                    the monitor resolution.\n",
      "Advancement of Tree Explorer\n",
      "Tree Explorer (TE) has been enhanced by adding a quick ac­\n",
      "                                                                    Additional GUI Updates\n",
      "cess panel on the side toolbar to provide easy access to custom­\n",
      "ization options previously accessible only through the menus        The MEGA12 GUI contains many custom forms to accommo­\n",
      "(see Fig. 3b). Searching tip names has been improved to facili­     date diverse analyses, results, and data exploration tools. In\n",
      "tate visualization and navigation through multiple matches.         MEGA12, a Windows menu has been added to all the data\n",
      "Users can now easily edit the names and fonts of the tip names      and result explorers, enabling users to navigate to any other\n",
      "in the phylogenetic tree, which can now be displayed with           currently active windows quickly. We have also made calcula­\n",
      "equalized branch lengths in TE or with tip names aligned ver­       tion progress reporting more informative, adding analysis de­\n",
      "tically. Labels for internal nodes and group names can now be       tails, calculated parameters, and data statistics. The display of\n",
      "edited directly in TE by right-clicking a given node. Clones of     some partial results has been programmed when a user issues a\n",
      "the Tree Explorer and current results can now be generated,         command to terminate long-running processes prematurely\n",
      "giving users snapshot copies of the current display as format­      but desires to see the results obtained thus far, such as the\n",
      "ting and other edits are made to one of the copies. Finally, dis­   bootstrap analysis. Finally, we have updated the Caption\n",
      "play settings between trees across tabs in TE have been             Expert system introduced in MEGA4 (Tamura et al. 2007)\n",
      "synchronized to align tree displays visually.                       to generate natural language descriptions of the models, meth­\n",
      "                                                                    ods, and parameters used in analyses. All the captions are up­\n",
      "                                                                    dated for brevity and clarity. An example caption is shown for\n",
      "Advancement of the Tree Topology Editor\n",
      "                                                                    a result from DrPhylo in Fig. 4d.\n",
      "MEGA offers functionality for manual drawing and editing a\n",
      "phylogeny, which can help update an existing tree by adding\n",
      "taxa and rearranging them through drag-and-drop opera­              Conclusion\n",
      "tions. The Tree Topology Editor in MEGA12 features several          We have described numerous major upgrades implemented in\n",
      "quality-of-life enhancements for manual editing of phyloge­         MEGA12, significantly enhancing its computational efficiency\n",
      "nies. Users can now assign branch lengths and node heights,         and useability. We expect many phylogenetic analyses using\n",
      "\f",
      "8                                                                                     Kumar et al. · https://doi.org/10.1093/molbev/msae263\n",
      "\n",
      "\n",
      "ML methods to finish more quickly than previous versions,                     1992:9:366–369. https://doi.org/10.1093/oxfordjournals.molbev.\n",
      "which is made possible by developing and implementing heu­                    a040725.\n",
      "ristics that avoid unnecessary computation during the selection            Kapli P, Yang Z, Telford MJ. Phylogenetic tree building in the genomic\n",
      "of optimal substitution models and bootstrap tests of phyl­                   age. Nat Rev Genet. 2020:21(7):428–444. https://doi.org/10.1038/\n",
      "                                                                              s41576-020-0233-0.\n",
      "ogeny. These heuristics were tested by analyzing many empiric­\n",
      "                                                                           Kumar S. Embracing green computing in molecular phylogenetics. Mol\n",
      "al datasets, and the results suggest that their use will generally\n",
      "                                                                              Biol Evol. 2022:39(3):msac043. https://doi.org/10.1093/molbev/\n",
      "produce the same result as this without using the heuristics.                 msac043.\n",
      "In the future, we plan to make MEGA even more computation­                 Kumar S. Embracing green computing in molecular phylogenetics. Mol\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                    Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "ally efficient, particularly for analyzing phylogenomic align­                Biol Evol. 2022:39:msac043.\n",
      "ments on desktop computers used by many MEGA users.                        Kumar S, Filipski AJ, Battistuzzi FU, Kosakovsky Pond SL, Tamura K.\n",
      "                                                                              Statistics and truth in phylogenomics. Mol Biol Evol. 2012a:29(2):\n",
      "                                                                              457–472. https://doi.org/10.1093/molbev/msr202.\n",
      "Supplementary Material                                                     Kumar S, Sanderford M, Gray VE, Ye J, Liu L. Evolutionary diagnosis\n",
      "Supplementary material is available at Molecular Biology and                  method for variants in personal exomes. Nat Methods. 2012b:9(9):\n",
      "Evolution online.                                                             855–856. https://doi.org/10.1038/nmeth.2147.\n",
      "                                                                           Kumar S, Sharma S. Evolutionary sparse learning for phylogenomics.\n",
      "                                                                              Mol Biol Evol. 2021:38(11):4674–4682. https://doi.org/10.1093/\n",
      "Acknowledgments                                                               molbev/msab227.\n",
      "The authors thank Dr. Alessandra Lamarca for analyzing data                Kumar S, Tamura K, Nei M. MEGA: molecular evolutionary genetics\n",
      "to test the initial tree search in the ML phylogeny construction.             analysis software for microcomputers. Comput Appl Biosci.\n",
      "                                                                              1994:10:189–191. https://doi.org/10.1093/bioinformatics/10.2.\n",
      "                                                                              189.\n",
      "Funding                                                                    Kumar S, Tamura K, Nei M. MEGA3: integrated software for mo­\n",
      "This work was supported by a research grant from the                          lecular evolutionary genetics analysis and sequence alignment.\n",
      "                                                                              Brief Bioinform. 2004:5(2):150–163. https://doi.org/10.1093/\n",
      "National Institutes of Health to S.K. (R35GM139540-04).\n",
      "                                                                              bib/5.2.150.\n",
      "                                                                           Liu L, Tamura K, Sanderford M, Gray VE, Kumar S. A molecular evo­\n",
      "Data Availability                                                             lutionary reference for the human variome. Mol Biol Evol.\n",
      "                                                                              2016:33(1):245–254. https://doi.org/10.1093/molbev/msv198.\n",
      "All sequence alignments and phylogenetic trees used in                     Nei M, Kumar S. Molecular evolution and phylogenetics. New York:\n",
      "this article were obtained from published articles and as­                    Oxford University Press; 2000.\n",
      "sembled in a Figshare repository: https://figshare.com/s/                  Pattengale ND, Alipour M, Bininda-Emonds ORP, Moret BME,\n",
      "0413dd262c2ed9df1bd2. MEGA12 can be downloaded                                Stamatakis A. How many bootstrap replicates are necessary?\n",
      "from https://www.megasoftware.net for use on MS Windows.                      J Comput Biol. 2010:17(3):337–354. https://doi.org/10.1089/cmb.\n",
      "An application for macOS is in the early testing and hardening                2009.0179.\n",
      "phase, which we hope to release soon. Linux releases will follow           Sanderford M, Sharma S, Tamura K, Stecher G, Liu J, Ji S, Ye J,\n",
      "them. The source code is available from https://github.com/                   Kumar S. 2024. MyESL: A software for evolutionary sparse learning\n",
      "KumarMEGALab/MEGA-source-code/tree/master/MEGA12-                             in molecular phylogenetics and genomics. Submitted [Internet].\n",
      "source.                                                                       [accessed 2024 Dec 14]. https://kumarlab.net/downloads/papers/\n",
      "                                                                              SanderfordKumar2024.pdf\n",
      "                                                                           Sharma S, Kumar S. Fast and accurate bootstrap confidence limits\n",
      "                                                                              on genome-scale phylogenies using little bootstraps. Nat Comput\n",
      "References                                                                    Sci. 2021:1(9):573–577. https://doi.org/10.1038/s43588-021-\n",
      "Abadi S, Azouri D, Pupko T, Mayrose I. Model selection may not be a           00129-5.\n",
      "    mandatory step for phylogeny reconstruction. Nat Commun.               Sharma S, Kumar S. Discovering fragile clades and causal sequences in\n",
      "    2019:10(1):1–11. https://doi.org/10.1038/s41467-019-08822-w.              phylogenomics by evolutionary sparse learning. Mol Biol Evol.\n",
      "Adachi J, Waddell PJ, Martin W, Hasegawa M. Plastid genome phyl­              2024:41(7):msae131. https://doi.org/10.1093/molbev/msae131.\n",
      "    ogeny and a model of amino acid substitution for proteins encoded      Sharma S, Kumar S. Discovering fragile clades and causal sequences in\n",
      "    by chloroplast DNA. J Mol Evol. 2000:50(4):348–358. https://doi.          phylogenomics by evolutionary sparse learning. Mol. Biol. Evol.\n",
      "    org/10.1007/s002399910038.                                                2024:41:msae131.\n",
      "Edgar RC. MUSCLE: a multiple sequence alignment method with re­            Shen XX, Hittinger CT, Rokas A. Contentious relationships in\n",
      "    duced time and space complexity. BMC Bioinformatics. 2004:5(1):           phylogenomic studies can be driven by a handful of genes. Nat\n",
      "    113. https://doi.org/10.1186/1471-2105-5-113.                             Ecol Evol. 2017:1(5):126. https://doi.org/10.1038/s41559-017-\n",
      "Felsenstein J. Confidence limits on phylogenies: an approach using the        0126.\n",
      "    bootstrap. Evolution. 1985:39(4):783–791. https://doi.org/10.2307/     Song S, Liu L, Edwards SV, Wu S. Resolving conflict in eutherian mam­\n",
      "    2408678.                                                                  mal phylogeny using phylogenomics and the multispecies coalescent\n",
      "Gadagkar SR, Rosenberg MS, Kumar S. Inferring species phylogenies             model. Proc Natl Acad Sci U S A. 2012:109(37):14942–14947.\n",
      "    from multiple genes: concatenated sequence tree versus consensus          https://doi.org/10.1073/pnas.1211733109.\n",
      "    gene tree. J Exp Zool B Mol Dev Evol. 2005:304(1):64–74.               Tamura K, Battistuzzi FU, Billing-Ross P, Murillo O, Filipski A, Kumar\n",
      "    https://doi.org/10.1002/jez.b.21026.                                      S. Estimating divergence times in large molecular phylogenies. Proc\n",
      "Gatesy J, Springer MS. Phylogenetic analysis at deep timescales: unreli­      Natl Acad Sci U S A. 2012:109(47):19333–19338. https://doi.org/\n",
      "    able gene trees, bypassed hidden support, and the coalescence/conca­      10.1073/pnas.1213199109.\n",
      "    talescence conundrum. Mol Phylogenet Evol. 2014:80:231–266.            Tamura K, Dudley J, Nei M, Kumar S. MEGA4: Molecular\n",
      "    https://doi.org/10.1016/j.ympev.2014.08.013.                              Evolutionary Genetics Analysis (MEGA) software version 4.0. Mol\n",
      "Hedges SB. The number of replications needed for accurate estimation          Biol Evol. 2007:24(8):1596–1599. https://doi.org/10.1093/molbev/\n",
      "    of the bootstrap P value in phylogenetic studies. Mol Biol Evol.          msm092.\n",
      "\f",
      "MEGA12 for Adaptive and Green Computing · https://doi.org/10.1093/molbev/msae263                                                               9\n",
      "\n",
      "\n",
      "Tamura K, Peterson D, Peterson N, Stecher G, Nei M, Kumar S.               Warnow T. Concatenation analyses in the presence of incomplete lineage\n",
      "  MEGA5: molecular evolutionary genetics analysis using maximum               sorting. PLoS Curr. 2015:7:ecurrents.currents.tol.8d41ac0f13d1ab\n",
      "  likelihood, evolutionary distance, and maximum parsimony meth­              edf4c4a59f5d17b1f7. 10.1371/currents.tol.8d41ac0f13d1abedf4c4a\n",
      "  ods. Mol Biol Evol. 2011:28(10):2731–2739. https://doi.org/10.              59f5d17b1f7.\n",
      "  1093/molbev/msr121.                                                      Williams TA, Cox CJ, Foster PG, Szöllő si GJ, Embley TM.\n",
      "Tamura K, Stecher G, Kumar S. 2021. MEGA11: Molecular Evolutionary            Phylogenomics provides robust support for a two-domains tree of\n",
      "  Genetics Analysis version 11. Mol Biol Evol. 38(7):3022–3027. https://      life. Nat Ecol Evol. 2020:4(1):138–147. https://doi.org/10.1038/\n",
      "  doi.org/10.1093/molbev/msab120.                                             s41559-019-1040-x.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                                    Downloaded from https://academic.oup.com/mbe/article/41/12/msae263/7930299 by Temple University Law School Library user on 11 June 2025\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7187c2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22792\n",
      "2352\n"
     ]
    }
   ],
   "source": [
    "print(len(data_tokens))\n",
    "print(len(set(data_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b23be3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### The 5000 tokens we got after tokenizing out personal data are not necessarily our vocabulary. They are the token instances (i.e., a stream of token IDs), not the vocabulary (i.e., the set of all unique tokens our tokenizer knows about).\n",
    "    \n",
    "#### Vocabulary is the complete set of unique tokens that the tokenizer (tiktoken) can recognize. If we are using a tokenizer like tiktoken.get_encoding(\"gpt2\"), its vocabulary size is fixed — 50,257 tokens in GPT-2's case. These include individual characters, subwords, or words, depending on the tokenizer training.\n",
    "    \n",
    "#### If we are training a Transformer From Scratch: There are two ways to go about it depending on what we are trying to achieve:\n",
    "\n",
    "#### Option 1: Use a Pretrained Tokenizer (tiktoken)\n",
    "#### Option 2: Want a Custom Vocabulary (train your own tokenizer)   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88defe7d",
   "metadata": {},
   "source": [
    "# Step 3 : Finalise the Input-Output Target pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ac75d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### So basically, training the LLM is like training an autoregressive model. Sentence structure is used for training. There are no labels and for the same reason it is called unsupervised learning.\n",
    "    \n",
    "#### Here, we introduce the concept of Context Window / Size i.e. the number of words / tokens that will be used to predict the next one.\n",
    "    \n",
    "#### For eg. text \"Hardik Sharma is a good boy.\" If the context window is 4, that means \"Hardik Sharma is a\" will be used to predict \"good\". \"Sharma is a good\" will be used to predict \"boy\". \n",
    "    \n",
    "#### For any given input with a context length length, the output will be another sequence just shifted by one.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3873365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT--> ['Hardik', 'Sharma', 'is', 'a']\n",
      "OUTPUT--> ['Sharma', 'is', 'a', 'good']\n",
      "-------------------------------------------\n",
      "['Hardik'] ---> Sharma\n",
      "['Hardik', 'Sharma'] ---> is\n",
      "['Hardik', 'Sharma', 'is'] ---> a\n",
      "['Hardik', 'Sharma', 'is', 'a'] ---> good\n",
      "['Hardik', 'Sharma', 'is', 'a', 'good'] ---> boy\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "text = \"Hardik Sharma is a good boy\"\n",
    "text=text.split()\n",
    "context_size = 4 \n",
    "print(\"INPUT-->\", text[:context_size])\n",
    "print(\"OUTPUT-->\", text[1:context_size+1])\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "# Each of the given is a prediction ask in itself.\n",
    "for i in range(len(text)-1):\n",
    "    print(f\"{text[:i+1]} ---> {text[i+1]}\")\n",
    "\n",
    "# We need to extract the given input-output pairs from our input text. \n",
    "# We will also introduce the concept of batches here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc674183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18db4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS USED FOR LOADING THE DATA\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    # METHOD 1 : HOW AND WHERE THE DATA SHOULD BE LOADED\n",
    "    def __init__(self, text_data, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text_data, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            target_chunk = token_ids[i + 1: i + context_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    # METHOD 2 : GIVE THE TOTAL NUMBER OF BATCHES \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # METHOD 3 : USED FOR FETCHING INFORMATION\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a98c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETLOADER CLASS USED FOR CREATING THE BATCHES\n",
    "# I just create a function here which will automatically define our dataset as well as dataloader\n",
    "\n",
    "def make_dataloader(text_data, batch_size=4, context_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    dataset = CustomDataset(text_data, tokenizer, context_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd00919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = make_dataloader(text_data, batch_size=1, context_length=5, stride=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23396ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[14567,   329,  1449, 21319,   290]]), tensor([[  329,  1449, 21319,   290, 16287]])]\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch) # prints the input batch and the output batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a61b8274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  530,   286,   262,   198,    83],\n",
      "        [  220,   220,   220, 48609,    25],\n",
      "        [  220,   220,   220,   220,   220],\n",
      "        [  583, 11111,   357,  6322,    40]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  286,   262,   198,    83,  1009],\n",
      "        [  220,   220, 48609,    25,  3901],\n",
      "        [  220,   220,   220,   220,   220],\n",
      "        [11111,   357,  6322,    40,     8]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = make_dataloader(text_data, batch_size=4, context_length=5, stride=5)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e4bdd",
   "metadata": {},
   "source": [
    "# Step 4 : Create the input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a58b96",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### For each token ID, we will represent them in a 'n' dimensional embedding vector. The embedding weights are initialized randomly which serves as the starting point for our LLM learning process. The embedding weights are optimized as a part of training process\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0091d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### THINK OF EMBEDDING WEIGHT LAYER AS A LOOKUP MATRIX THAT CONTAINS EMBEDDING LAYER FOR EACH TOKEN ID. THEREFORE, THE DIMENSION OF AN EMBEDDING MATRIX = Number of token ids x 'n' Dimension. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8718dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9978,  1.0415,  2.0103,  0.0099, -2.1790],\n",
      "        [ 0.9618, -0.6465,  0.5149, -1.8752,  0.6310],\n",
      "        [ 1.1875,  0.2682, -1.5297, -0.4929, -1.7336],\n",
      "        [ 0.0338, -2.3538,  2.6719,  2.7228,  1.8366],\n",
      "        [ 0.3175, -1.0971,  0.3268, -1.2724, -0.7179]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE : WE HAVE 5 TOKENS, EACH IS REPRESENT THROUGH A 5 DIMENSION VECTOR\n",
    "\n",
    "num_tokens = 5 # For eg. \"Hardik\",\"is\",\"a\",\"good\",\"boy\"\n",
    "embedding_dim = 5\n",
    "embedding_matrix = torch.nn.Embedding(num_tokens, embedding_dim)\n",
    "\n",
    "print(embedding_matrix.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090e096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0338, -2.3538,  2.6719,  2.7228,  1.8366]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_id = torch.tensor([3])\n",
    "print(embedding_matrix(token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b710f5d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### Since we have 50257 tokens in our vocabulary (Tiktoken GPT2 vocabulary), the num_tokens = 50257.\n",
    "#### Let's start with the embedding_dim to be 128, i.e. each token will be represented as a 128 dimension vector.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1502ecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 128)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 50257\n",
    "embedding_dim = 128\n",
    "embedding_matrix = torch.nn.Embedding(num_tokens, embedding_dim)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb66513",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Using the embedding_matrix above, if we sample data from the data loader, we embed each token in each batch into a 128-dimensional vector. If we have a batch size of 4 with 5 tokens each, the result will be an 4 x 5 x 128 tensor.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "221745c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = make_dataloader(text_data, batch_size=4, context_length=5, stride=5)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c4a02c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = embedding_matrix(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83526c22",
   "metadata": {},
   "source": [
    "# Step 5 : Create the positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced07644",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### Untill now, we converted the input sequence into vectors, but we didn't account for each tokens position inside the sentence. Let us work on that.\n",
    "#### Positional embeddings can be 1. Absolute 2. Relative\n",
    "    \n",
    "#### Absolute are used when fixed order of tokens is crucial like sequence generation. Relative embeddings are used when performing language modelling, over long sequences where some phrase can appear in different parts of the sequence.\n",
    "    \n",
    "#### Positional encodings are always of size context length x embedding dimension\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cb696e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc7acbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "pos_embedding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a4b0347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "# INPUT EMBEDDINGS = TOKEN_EMBEDDINGS + POSITIONAL_EMBEDDINGS\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608c450",
   "metadata": {},
   "source": [
    "# Step 6 : Transformer Block\n",
    "\n",
    "## Step 6.1 : Self Attention without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf18d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor( \n",
    "  [[0.43, 0.15, 0.89],  # HARDIK    \n",
    "   [0.55, 0.87, 0.66],  # IS  \n",
    "   [0.57, 0.85, 0.64],  # A   \n",
    "   [0.22, 0.58, 0.33],  # GOOD     \n",
    "   [0.77, 0.25, 0.10]]  # BOY    \n",
    ")\n",
    "# Input Shape 5 X 3\n",
    "# Self Attention shape 5 X 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a8603",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### Self attention is a mechanism that allows each position of the input sequence to attend to all positions in the same sequence when computing the respresentation.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f60400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654]])\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "self_attention_score = inputs @ inputs.T\n",
    "print(self_attention_score)\n",
    "print(self_attention_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8688a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### Next, we normalize the attentions scores so that they add upto 1. It is useful for interpretation and also for maintaining the stability in an LLM. Simplest way is to sum up the total and divide by the total but it is advisable to use softmax function from the torch.\n",
    "\n",
    "#### In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies the dimension of the input tensor along which the function will be computed. By setting dim=-1, we are instructing the softmax function to apply the normalization along the last dimension of the attn_scores tensor. If attn_scores is a 2D tensor (for example, with a shape of [rows, columns]), dim=-1 will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bb004d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2455, 0.2346, 0.2318, 0.1453, 0.1428],\n",
      "        [0.1646, 0.2826, 0.2771, 0.1473, 0.1285],\n",
      "        [0.1648, 0.2809, 0.2757, 0.1472, 0.1314],\n",
      "        [0.1733, 0.2505, 0.2471, 0.1766, 0.1525],\n",
      "        [0.1753, 0.2250, 0.2269, 0.1570, 0.2158]])\n"
     ]
    }
   ],
   "source": [
    "self_attention_weights = torch.softmax(self_attention_score, dim=-1)\n",
    "print(self_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65ae4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "#### Context vector for our input is self attention weights x input\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00c2d3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5086, 0.5580, 0.5839],\n",
      "        [0.5155, 0.6236, 0.5717],\n",
      "        [0.5160, 0.6217, 0.5702],\n",
      "        [0.5094, 0.5945, 0.5512],\n",
      "        [0.5292, 0.5599, 0.5231]])\n"
     ]
    }
   ],
   "source": [
    "context_vector = self_attention_weights @ inputs\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828bbea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Currently the above attention mechanism is without any trainable weights. Just random static values. This is not an effective approach. In the next section we will introduce attention mechanism is with trainable weights. We will introduce 3 matrices namely Query, Key and Value.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596b4ac",
   "metadata": {},
   "source": [
    "## Step 6.2 : Self Attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b818be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Each of the above weight matrices Query, Key and Value the dimension is input_dim x output_dim. Input_dim is generally equal to the vector dimension of input. The output_dim can be anything. Generally it is equal to the input_dim \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e3252ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor( \n",
    "  [[0.43, 0.15, 0.89],  # HARDIK    \n",
    "   [0.55, 0.87, 0.66],  # IS  \n",
    "   [0.57, 0.85, 0.64],  # A   \n",
    "   [0.22, 0.58, 0.33],  # GOOD     \n",
    "   [0.77, 0.25, 0.10]]  # BOY    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4dbb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fc24d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query = torch.nn.Parameter(torch.rand(input_dim, output_dim), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(input_dim, output_dim), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(input_dim, output_dim), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd06467d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.2428, 0.9517, 0.1209],\n",
       "         [0.8577, 0.6965, 0.5769],\n",
       "         [0.5285, 0.7489, 0.4559]]),\n",
       " Parameter containing:\n",
       " tensor([[0.1399, 0.3710, 0.1960],\n",
       "         [0.8147, 0.9836, 0.9322],\n",
       "         [0.1146, 0.3451, 0.7438]]),\n",
       " Parameter containing:\n",
       " tensor([[0.0768, 0.2979, 0.5673],\n",
       "         [0.5852, 0.8943, 0.4426],\n",
       "         [0.7718, 0.4632, 0.0711]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query, W_key, W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0911c575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys.shape: torch.Size([5, 3])\n",
      "Values.shape: torch.Size([5, 3])\n",
      "Queries.shape: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "Queries = inputs @ W_query\n",
    "Keys = inputs @ W_key\n",
    "Values = inputs @ W_value\n",
    "\n",
    "print(\"Keys.shape:\", Keys.shape)\n",
    "print(\"Values.shape:\", Values.shape)\n",
    "print(\"Queries.shape:\", Queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e144f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = Keys.shape[-1]\n",
    "self_attention_score = ((Queries @ Keys.T)/d_k**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92639aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3771ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_weights = torch.softmax(self_attention_score, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee09f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5132, 0.6563, 0.5698],\n",
      "        [0.5192, 0.7084, 0.5896],\n",
      "        [0.5189, 0.7065, 0.5889],\n",
      "        [0.5113, 0.6402, 0.5636],\n",
      "        [0.5110, 0.6240, 0.5561]])\n"
     ]
    }
   ],
   "source": [
    "context_vector = self_attention_weights @ inputs\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408ee97",
   "metadata": {},
   "source": [
    "### Step 6.2.1 : Summarize the above steps in a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36ef65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Queries = self.W_query(inputs)\n",
    "        Keys = self.W_key(inputs)\n",
    "        Values = self.W_value(inputs)\n",
    "        \n",
    "        self_attention_score = Queries @ Keys.T\n",
    "        self_attention_weights = torch.softmax(self_attention_score / Keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vector = self_attention_weights @ Values\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46dbffb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1150,  0.0186, -0.0135],\n",
      "        [-0.1192,  0.0206, -0.0111],\n",
      "        [-0.1191,  0.0206, -0.0113],\n",
      "        [-0.1145,  0.0196, -0.0157],\n",
      "        [-0.1138,  0.0213, -0.0193]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttention(input_dim, output_dim)\n",
    "print(self_attention(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812eff1f",
   "metadata": {},
   "source": [
    "## Step 6.3 : Adding Causal Attention mechanism for our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cbc8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 5\n",
    "mask = torch.triu(torch.ones([context_length,context_length]), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a0f1a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b67a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8124, 1.6702, 1.6414, 1.0023, 0.6609],\n",
      "        [1.2222, 2.5256, 2.4815, 1.5181, 0.9897],\n",
      "        [1.2056, 2.4920, 2.4486, 1.4979, 0.9774],\n",
      "        [0.6860, 1.4168, 1.3920, 0.8521, 0.5518],\n",
      "        [0.5675, 1.1860, 1.1662, 0.7116, 0.4804]])\n"
     ]
    }
   ],
   "source": [
    "print(self_attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d1eef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8124,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2222, 2.5256,   -inf,   -inf,   -inf],\n",
      "        [1.2056, 2.4920, 2.4486,   -inf,   -inf],\n",
      "        [0.6860, 1.4168, 1.3920, 0.8521,   -inf],\n",
      "        [0.5675, 1.1860, 1.1662, 0.7116, 0.4804]])\n"
     ]
    }
   ],
   "source": [
    "masked_self_attention_score = self_attention_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked_self_attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf6edb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3203, 0.6797, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1941, 0.4080, 0.3979, 0.0000, 0.0000],\n",
      "        [0.1950, 0.2973, 0.2931, 0.2146, 0.0000],\n",
      "        [0.1701, 0.2431, 0.2403, 0.1848, 0.1617]])\n"
     ]
    }
   ],
   "source": [
    "masked_self_attention_weights = torch.softmax(masked_self_attention_score / Keys.shape[-1]**0.5, dim=1)\n",
    "print(masked_self_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ebb5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WE ALSO ADD DROPUT IN THE GIVEN ARCHITECTURE AS IT PREVENTS OVERFITTING BY MASKING RANDOM POSITIONS\n",
    "dropout = nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94ba5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dropout(masked_self_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7756c70",
   "metadata": {},
   "source": [
    "### Step 6.3.1 : Summarize the above causal attention steps in a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7942784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, context_length, p, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Queries = self.W_query(inputs)\n",
    "        Keys = self.W_key(inputs)\n",
    "        Values = self.W_value(inputs)\n",
    "        \n",
    "        batch, num_tokens, input_dim = inputs.shape\n",
    "        print(f\"Batches = {batch}, Context Size = {num_tokens}, Embedding dimension = {input_dim}\")\n",
    "        self_attention_score = Queries @ Keys.transpose(1,2)\n",
    "        \n",
    "        masked_self_attention_score = self_attention_score.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        masked_self_attention_weights = torch.softmax(masked_self_attention_score / Keys.shape[-1]**0.5, dim=-1)\n",
    "        masked_self_attention_weights = self.dropout(masked_self_attention_weights)\n",
    "        \n",
    "        context_vector = masked_self_attention_weights @ Values\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "378a498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING OUR CLASS WITH A TEST INPUT OF Batches = 2, Context Size = 3, Embedding dimension = 5\n",
    "test_input = torch.rand(2,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0ecf53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2367, 0.6611, 0.9241, 0.5598, 0.5368],\n",
       "         [0.4707, 0.0966, 0.8298, 0.6292, 0.5655],\n",
       "         [0.6974, 0.2714, 0.3311, 0.5154, 0.8852]],\n",
       "\n",
       "        [[0.8127, 0.7228, 0.9167, 0.7630, 0.6656],\n",
       "         [0.4599, 0.1709, 0.2292, 0.7050, 0.0946],\n",
       "         [0.1564, 0.0611, 0.6763, 0.9193, 0.1807]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a5b53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "output_dim = 5\n",
    "context_length = 3\n",
    "p = 0.5\n",
    "causalattention = CausalAttention(input_dim,output_dim,context_length,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34f5359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches = 2, Context Size = 3, Embedding dimension = 5\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1723,  0.4552, -0.1450,  0.0111, -0.6443],\n",
      "         [-0.1902,  0.2381,  0.0229, -0.0514, -0.4299]],\n",
      "\n",
      "        [[-0.4189,  1.0184, -0.0397, -0.0673, -1.5546],\n",
      "         [-0.0317,  0.3413, -0.2110,  0.2650, -0.2990],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cv = causalattention(test_input)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050271c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### The outputs acheived from the above causal attention class can be considered as an output from 1 head. If we carry this out for multiple times, that is, multiple heads, it is called multi-head attention. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d33a1a",
   "metadata": {},
   "source": [
    "## Step 6.4 : Adding naive Multi-head Attention mechanism for our architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168c990",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### In the following code, we can achieve a naive MultiHeadAttention class by stacking multiple instances of our CausalAttention class module\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "485bd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionNaive(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, context_length, p, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(input_dim, output_dim, context_length, p, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef2d82ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches = 2, Context Size = 3, Embedding dimension = 5\n",
      "Batches = 2, Context Size = 3, Embedding dimension = 5\n",
      "tensor([[[-1.0645,  0.1453,  0.3904, -0.1668, -0.0453,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000],\n",
      "         [-0.5353,  0.0731,  0.1963, -0.0839, -0.0228, -0.4796, -0.1422,\n",
      "           0.3841,  0.0086,  0.1783],\n",
      "         [-0.4199,  0.0559,  0.3297, -0.1728, -0.0359, -0.9099, -0.3962,\n",
      "           0.6297,  0.1122, -0.2356]],\n",
      "\n",
      "        [[-1.0958, -0.1064,  0.7098, -0.4348, -0.0073, -0.9322, -0.4943,\n",
      "           0.9065, -0.0907, -0.1938],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4191, -0.2222,\n",
      "           0.4075, -0.0408, -0.0871],\n",
      "         [-0.5532,  0.0020,  0.4718, -0.2133,  0.0884, -0.8454, -0.4479,\n",
      "           0.4909, -0.2812,  0.0110]]], grad_fn=<CatBackward0>)\n",
      "torch.Size([2, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 5\n",
    "output_dim = 5\n",
    "context_length = 3\n",
    "p = 0.5\n",
    "num_heads = 2\n",
    "multiheadattention = MultiHeadAttentionNaive(input_dim,output_dim,context_length,p, num_heads)\n",
    "mha = multiheadattention(test_input)\n",
    "print(mha)\n",
    "print(mha.shape)\n",
    "\n",
    "## THE FINAL CONTEXT VECTOR WILL OF THE DIMENSION (BATCH x CONTEXT_LENGTH x EMBEDDING DIMENSION*NUM_HEADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9f0c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### The above class does the job for us, but this is not optimal as the above will run sequentially but we want parallel execution of our code.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7dd222",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "#### Instead of maintaining two separate classes, MultiHeadAttention and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class. Also, in addition to just merging the MultiHeadAttentionWrapper with the CausalAttention code, we will make some other modifications to implement multi-head attention more efficiently.\n",
    "    \n",
    "#### In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list of CausalAttention objects (self.heads), each representing a separate attention head. The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81715dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEFORE FEEDING INPUT INTO THE MULTI-HEAD ATTENTION CLASS DECIDE THE FOLLOWING 2 THINGS:\n",
    "## OUTPUT_DIM AND NUM_HEADS\n",
    "## THERE IS FORMULA FOR CALCULATING HEAD DIMENSION = OUTPUT_DIM / NUM_HEADS\n",
    "## THEREFORE, THE OUTPUT_DIM % NUM_HEADS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3809813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, context_length, p, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (output_dim % num_heads == 0), \\\n",
    "            \"output_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = output_dim // num_heads\n",
    "        \n",
    "        self.W_query = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.Output_Projection = nn.Linear(output_dim, output_dim) \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch, num_tokens, input_dim = inputs.shape\n",
    "        \n",
    "        Queries = self.W_query(inputs)\n",
    "        Keys = self.W_key(inputs)\n",
    "        Values = self.W_value(inputs)\n",
    "        \n",
    "        Keys = Keys.view(batch, num_tokens, self.num_heads, self.head_dim) \n",
    "        Queries = Queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        Values = Values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        Keys = Keys.transpose(1, 2)\n",
    "        Queries = Queries.transpose(1, 2)\n",
    "        Values = Values.transpose(1, 2)\n",
    "\n",
    "        self_attention_score = Queries @ Keys.transpose(2, 3) \n",
    "\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        masked_self_attention_score = self_attention_score.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        masked_self_attention_weights = torch.softmax(masked_self_attention_score / Keys.shape[-1]**0.5, dim=-1)\n",
    "        masked_self_attention_weights = self.dropout(masked_self_attention_weights)\n",
    "\n",
    "        context_vector = (masked_self_attention_weights @ Values).transpose(1, 2) \n",
    "        \n",
    "        context_vector = context_vector.contiguous().view(batch, num_tokens, self.output_dim)\n",
    "        context_vector = self.Output_Projection(context_vector) # optional projection\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb28ccb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Step 1: Reduce the projection dim to match desired output dim\n",
    "\n",
    "#### Step 2: Use a Linear layer to combine head outputs\n",
    "\n",
    "#### Step 3: Tensor shape: (batch,num_tokens, output_dim)\n",
    "\n",
    "#### Step 4: We implicitly split the matrix by adding a \"num_heads\" dimension. Then we unroll last dim: (batch,num_tokens, output_dim) -> (batch, num_tokens, num_heads, head_dim)\n",
    "\n",
    "#### Step 5: Transpose from shape (batch, num_tokens, num_heads, head_dim) to (batch, num_heads, num_tokens, head_dim)\n",
    "\n",
    "#### Step 6: Compute dot product for each head\n",
    "\n",
    "#### Step 7: Mask truncated to the number of tokens\n",
    "\n",
    "#### Step 8: Use the mask to fill attention scores\n",
    "\n",
    "#### Step 9: Tensor shape: (batch, num_tokens, n_heads, head_dim)\n",
    "\n",
    "#### Step 10: Combine heads, where self.output_dim = self.num_heads * self.head_dim\n",
    "\n",
    "#### Step 11: Add an optional linear projection\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86e4ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.rand(2,3,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35ab4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1718, -0.1055, -0.3672, -0.0650,  0.2676,  0.0720],\n",
      "         [ 0.4229, -0.2793, -0.3730, -0.0273,  0.3053, -0.0127],\n",
      "         [ 0.2311, -0.1652, -0.3586, -0.0664,  0.2687,  0.0118]],\n",
      "\n",
      "        [[ 0.2365, -0.2665, -0.4015,  0.1143,  0.5299,  0.1356],\n",
      "         [ 0.4784, -0.3644, -0.2417, -0.0920,  0.1963,  0.1256],\n",
      "         [ 0.5203, -0.4821, -0.2122,  0.0179,  0.3599,  0.1866]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 6\n",
    "output_dim = 6\n",
    "context_length = 3\n",
    "p = 0.5\n",
    "num_heads = 2\n",
    "multiheadattention = MultiHeadAttention(input_dim,output_dim,context_length,p, num_heads)\n",
    "mha = multiheadattention(test_input)\n",
    "print(mha)\n",
    "print(mha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e6fe5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOW THAT WE HAVE CODED OUR ENTIRE MULTIHEAD ATTENTION CLASS. LET US CREATE OTHER COMPONENTS.\n",
    "## OTHER COMPONENTS : LAYER NORMALISATION, FEEDFORWARD, DROPOUT AND RESIDUAL CONNECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6a8f5",
   "metadata": {},
   "source": [
    "## Step 6.5 : Create a Configuration for  smalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23d4e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalLM_config= {\n",
    "    \"vocab_size\": 50257,          # Vocabulary size\n",
    "    \"context_length\": 100,        # Context length\n",
    "    \"embedding_dim\": 100,         # Embedding dimension\n",
    "    \"num_heads\": 1,               # Number of attention heads\n",
    "    \"num_layers\": 1,              # Number of layers\n",
    "    \"dropout_rate\": 0.5,          # Dropout rate\n",
    "    \"qkv_bias\": False             # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6533eea",
   "metadata": {},
   "source": [
    "## Step 6.6 : Coding the LayerNomalization Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b026377",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### The following implementation of Layer Normalization operates on the last dimension of the input tensor, which represents the embedding dimension (embedding_dim). The variable eps is a small constant (epsilon) added to the variance to prevent division by zero during normalization. The scale and shift are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task. This allows the model to learn appropriate scaling and shifting that best suit the data it is processing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3d48853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNomalization(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.eps =  1e-7\n",
    "        self.scale = nn.Parameter(torch.ones(embedding_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embedding_dim))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        mean = inputs.mean(dim=-1, keepdim=True)\n",
    "        var = inputs.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized_inputs = (inputs - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * normalized_inputs + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d58a006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.rand(2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48813420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0243, 0.4137, 0.4180],\n",
       "         [0.6813, 0.0193, 0.7872]],\n",
       "\n",
       "        [[0.3561, 0.3991, 0.7984],\n",
       "         [0.4015, 0.3860, 0.3515]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27aef887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4141,  0.6954,  0.7188],\n",
      "         [ 0.5455, -1.4027,  0.8572]],\n",
      "\n",
      "        [[-0.8123, -0.5965,  1.4087],\n",
      "         [ 1.0439,  0.3041, -1.3480]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNomalization(embedding_dim=3)\n",
    "print(ln(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b3fa4",
   "metadata": {},
   "source": [
    "## Step 6.6 : Coding the Feed Forward Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461752b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### FeedForward module is a small neural network consisting of two Linear layers and a GELU activation function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c55205",
   "metadata": {},
   "source": [
    "### Step 6.6.1 : GeLU activation token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e438efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        gelu = 0.5 * inputs * (1 + torch.tanh(torch.sqrt(torch.tensor(2/torch.pi)) * \n",
    "                                              (inputs + 0.044715*torch.pow(inputs,3))))\n",
    "        return gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "547b4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu = GeLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbec7ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0124, 0.2732, 0.2768],\n",
      "         [0.5124, 0.0098, 0.6175]],\n",
      "\n",
      "        [[0.2276, 0.2614, 0.6288],\n",
      "         [0.2633, 0.2510, 0.2240]]])\n"
     ]
    }
   ],
   "source": [
    "print(gelu(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b60801",
   "metadata": {},
   "source": [
    "### Step 6.6.2 : Feed Forward Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "639bef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,config_dict):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(config_dict[\"embedding_dim\"], 4*config_dict[\"embedding_dim\"]),\n",
    "        GeLU(),\n",
    "        nn.Linear(4*config_dict[\"embedding_dim\"],config_dict[\"embedding_dim\"]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.layers(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6df7504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(smalLM_config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04ed4b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 100])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.rand(2,3,100)\n",
    "feedforward =  FeedForward(smalLM_config)\n",
    "output = feedforward(test_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bf066",
   "metadata": {},
   "source": [
    "## Step 6.7 : Shortcut connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a464d11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Shortcut connections are important for overcoming the limitations posed by the vanishing gradient problem in deep neural networks.Shortcut connections are a core building block of very large models such as LLMs, and they will help facilitate more effective training by ensuring consistent gradient flow across layers when we train the GPT model\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92e72a",
   "metadata": {},
   "source": [
    "## Step 6.8 : Code the entire Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd6e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalLM_config= {\n",
    "    \"vocab_size\": 50257,          # Vocabulary size\n",
    "    \"context_length\": 100,        # Context length\n",
    "    \"embedding_dim\": 100,         # Embedding dimension\n",
    "    \"num_heads\": 1,               # Number of attention heads\n",
    "    \"num_layers\": 1,              # Number of layers\n",
    "    \"dropout_rate\": 0.5,          # Dropout rate\n",
    "    \"qkv_bias\": False             # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6d48fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(\n",
    "            config_dict[\"embedding_dim\"],\n",
    "            config_dict[\"embedding_dim\"],\n",
    "            config_dict[\"context_length\"],\n",
    "            config_dict[\"dropout_rate\"], \n",
    "            config_dict[\"num_heads\"])\n",
    "        \n",
    "        self.layernorm1 = LayerNomalization(config_dict[\"embedding_dim\"])\n",
    "        self.layernorm2 = LayerNomalization(config_dict[\"embedding_dim\"])\n",
    "        self.feedforward = FeedForward(config_dict)\n",
    "        self.dropout = nn.Dropout(config_dict[\"dropout_rate\"])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        shortcut = inputs\n",
    "        inputs = self.layernorm1(inputs)\n",
    "        inputs = self.attention(inputs)\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = inputs + shortcut\n",
    "        \n",
    "        shortcut = inputs\n",
    "        inputs = self.layernorm2(inputs)\n",
    "        inputs = self.feedforward(inputs)\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = inputs + shortcut\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027e42a",
   "metadata": {},
   "source": [
    "# Step 7 : Code the smalLM_model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "14d8b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class smalLM_model(nn.Module):\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config_dict[\"vocab_size\"], config_dict[\"embedding_dim\"])\n",
    "        self.position_embedding = nn.Embedding(config_dict[\"context_length\"], config_dict[\"embedding_dim\"])\n",
    "        self.dropout_embedding = nn.Dropout(config_dict[\"dropout_rate\"])\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config_dict) for _ in range(config_dict[\"num_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNomalization(config_dict[\"embedding_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            config_dict[\"embedding_dim\"], config_dict[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        batch_size, seq_len = input_idx.shape\n",
    "        tok_embeddings = self.token_embedding(input_idx)\n",
    "        pos_embeddings = self.position_embedding(torch.arange(seq_len, device=input_idx.device))\n",
    "        inputs = tok_embeddings + pos_embeddings  \n",
    "        inputs = self.dropout_embedding(inputs)\n",
    "        inputs = self.transformer_blocks(inputs)\n",
    "        inputs = self.final_norm(inputs)\n",
    "        logits = self.out_head(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c52867ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smalLM_model(smalLM_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "98aac9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 10,182,600\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bfc5a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 100])\n",
      "Output layer shape: torch.Size([50257, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.token_embedding.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f80d335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 38.84 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 \n",
    "total_size_mb = total_size_bytes / (1024 * 1024) \n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21dc2f",
   "metadata": {},
   "source": [
    "# Step 8 : Try to generate new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "427246d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO GENERATE TEXT\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "        logits = logits[:, -1, :]  \n",
    "        probas = torch.softmax(logits, dim=-1)  \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8658dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " After the storm passed, the villagers discovered something very strangeigningSK Dominion Werewolf governments SF paintedappyendeful coastline enjoysguestead swast compose tabsiletply Afghan\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# FUNCTION TO CONVERT A GIVEN TEXT INTO TOKENS\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "    return encoded_tensor\n",
    "\n",
    "# FUNCTION TO CONVERT A GIVEN TOKEN INTO TEXT\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"After the storm passed, the villagers discovered something very strange\"\n",
    "\n",
    "# CALL THE GENERATE TEXT FUNCTION TO GENERATE THE TEXT GIVEN A PARTICULAR START CONTEXT\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=20,\n",
    "    context_size=smalLM_config[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e71ab",
   "metadata": {},
   "source": [
    "# Step 9 : Code the entire Input to Output with Training Loop pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84f6b851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install pymupdf pdfplumber PyPDF2 pdf2image\n",
    "# import fitz  # PyMuPDF\n",
    "# with fitz.open(\"mega.pdf\") as doc:\n",
    "#     text = \"\"\n",
    "#     for page in doc:\n",
    "#         text += page.get_text()\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0136bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import importlib\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9ddf020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "Total number of tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "print(\"Total number of character:\", len(text_data))\n",
    "data_tokens = tokenizer.encode(text_data)\n",
    "print(\"Total number of tokens:\", len(data_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e9b465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS USED FOR LOADING THE DATA\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    # METHOD 1 : HOW AND WHERE THE DATA SHOULD BE LOADED\n",
    "    def __init__(self, text_data, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text_data, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            target_chunk = token_ids[i + 1: i + context_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    # METHOD 2 : GIVE THE TOTAL NUMBER OF BATCHES \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # METHOD 3 : USED FOR FETCHING INFORMATION\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4bec32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETLOADER CLASS USED FOR CREATING THE BATCHES\n",
    "# I just create a function here which will automatically define our dataset as well as dataloader\n",
    "def make_dataloader(text_data, batch_size=4, context_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    dataset = CustomDataset(text_data, tokenizer, context_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c577fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalLM_config= {\n",
    "    \"vocab_size\": 50257,          # Vocabulary size\n",
    "    \"context_length\": 100,        # Context length\n",
    "    \"embedding_dim\": 200,         # Embedding dimension\n",
    "    \"num_heads\": 10,               # Number of attention heads\n",
    "    \"num_layers\": 10,              # Number of layers\n",
    "    \"dropout_rate\": 0.5,          # Dropout rate\n",
    "    \"qkv_bias\": False             # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c407137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "train_dataloader = make_dataloader(train_data,\n",
    "                            batch_size = 4, \n",
    "                            context_length = smalLM_config[\"context_length\"],\n",
    "                            stride = 100,\n",
    "                            shuffle = True,\n",
    "                            drop_last = True,\n",
    "                            num_workers = 0)\n",
    "\n",
    "val_dataloader = make_dataloader(val_data,\n",
    "                            batch_size = 4, \n",
    "                            context_length = smalLM_config[\"context_length\"],\n",
    "                            stride = 100,\n",
    "                            shuffle = True,\n",
    "                            drop_last = True,\n",
    "                            num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59657c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK TO SEE WE HAVE ENOUGH DATA TO TRAIN ON\n",
    "# WE CAN ALSO PUT A CHECK LATER TO CHECK ON THE NUMBER OF ENOUGH TRAINING BATCHES\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "if total_tokens * (train_ratio) < smalLM_config[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the context_length or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < smalLM_config[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the context_length or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f384e29d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([4, 100]) torch.Size([4, 100])\n",
      "11\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e22b46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4400\n",
      "Validation tokens: 400\n",
      "All tokens: 4800\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_dataloader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_dataloader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8653e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class smalLM_model(nn.Module):\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config_dict[\"vocab_size\"], config_dict[\"embedding_dim\"])\n",
    "        self.position_embedding = nn.Embedding(config_dict[\"context_length\"], config_dict[\"embedding_dim\"])\n",
    "        self.dropout_embedding = nn.Dropout(config_dict[\"dropout_rate\"])\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config_dict) for _ in range(config_dict[\"num_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNomalization(config_dict[\"embedding_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            config_dict[\"embedding_dim\"], config_dict[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        batch_size, seq_len = input_idx.shape\n",
    "        tok_embeddings = self.token_embedding(input_idx)\n",
    "        pos_embeddings = self.position_embedding(torch.arange(seq_len, device=input_idx.device))\n",
    "        inputs = tok_embeddings + pos_embeddings  \n",
    "        inputs = self.dropout_embedding(inputs)\n",
    "        inputs = self.transformer_blocks(inputs)\n",
    "        inputs = self.final_norm(inputs)\n",
    "        logits = self.out_head(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43cf2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smalLM_model(smalLM_config)\n",
    "model.eval(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0c2ca42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 24,943,200\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83d25c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION IS USED TO CALCULATE THE CROSS ENTROPY LOSS BETWEEN THE TRUE AND THE PREDICTED TOKENS\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # PREDICTED AND TRUE\n",
    "    return loss\n",
    "\n",
    "\n",
    "# AGGREGRATE THE LOSSES FROM ALL THE BATCHES TO GET MEAN LOSS\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a8fc6d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esl/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.025489286942916\n",
      "Validation loss: 10.990496635437012\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Note:\n",
    "# # Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# # which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# # However, the resulting loss values may be slightly different.\n",
    "\n",
    "# #if torch.cuda.is_available():\n",
    "# #    device = torch.device(\"cuda\")\n",
    "# #elif torch.backends.mps.is_available():\n",
    "# #    device = torch.device(\"mps\")\n",
    "# #else:\n",
    "# #    device = torch.device(\"cpu\")\n",
    "# #\n",
    "# # print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "# torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_dataloader, model, device)\n",
    "    val_loss = calc_loss_loader(val_dataloader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e1771442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # COMPLETELY OPTIONAL\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2c83bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d497033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.position_embedding.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f96ead89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.843, Val loss 10.908\n",
      "Ep 1 (Step 000005): Train loss 10.096, Val loss 10.256\n",
      "Ep 1 (Step 000010): Train loss 9.605, Val loss 9.656\n",
      "Every effort moves you  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000015): Train loss 9.061, Val loss 9.162\n",
      "Ep 2 (Step 000020): Train loss 8.492, Val loss 8.640\n",
      "Every effort moves you  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the, the the the the the the the the the the the the the the the the the the\n",
      "Ep 3 (Step 000025): Train loss 7.828, Val loss 8.058\n",
      "Ep 3 (Step 000030): Train loss 7.364, Val loss 7.624\n",
      "Every effort moves you  the the the the the the the the the the, the the the the the, the the the the the the the the, the the the the the, the the the the the the the the the the the the the the the the the the\n",
      "Ep 4 (Step 000035): Train loss 6.943, Val loss 7.283\n",
      "Ep 4 (Step 000040): Train loss 6.614, Val loss 7.014\n",
      "Every effort moves you  the the the the the the the the the the, the the the the the, the the the the the the the the the the the the the the, the the the the the the the the the the the the the the the the the the\n",
      "Ep 5 (Step 000045): Train loss 6.354, Val loss 6.869\n",
      "Ep 5 (Step 000050): Train loss 6.276, Val loss 6.713\n",
      "Every effort moves you , the,.,.                                            \n",
      "Ep 6 (Step 000055): Train loss 6.113, Val loss 6.716\n",
      "Ep 6 (Step 000060): Train loss 6.139, Val loss 6.677\n",
      "Ep 6 (Step 000065): Train loss 6.131, Val loss 6.681\n",
      "Every effort moves you ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 7 (Step 000070): Train loss 6.067, Val loss 6.675\n",
      "Ep 7 (Step 000075): Train loss 6.068, Val loss 6.600\n",
      "Every effort moves you ,,,,,,, the,,, the,,,,,,,, the,, the,,, the,,,, the,,,, the,,,,,,,, the, the,\n",
      "Ep 8 (Step 000080): Train loss 6.025, Val loss 6.636\n",
      "Ep 8 (Step 000085): Train loss 5.932, Val loss 6.615\n",
      "Every effort moves you ,.                                                \n",
      "Ep 9 (Step 000090): Train loss 5.842, Val loss 6.630\n",
      "Ep 9 (Step 000095): Train loss 5.906, Val loss 6.627\n",
      "Every effort moves you , the the the the.                                            \n",
      "Ep 10 (Step 000100): Train loss 5.867, Val loss 6.560\n",
      "Ep 10 (Step 000105): Train loss 5.782, Val loss 6.620\n",
      "Every effort moves you , the.                                               \n",
      "Ep 11 (Step 000110): Train loss 5.739, Val loss 6.633\n",
      "Ep 11 (Step 000115): Train loss 5.656, Val loss 6.626\n",
      "Ep 11 (Step 000120): Train loss 5.596, Val loss 6.536\n",
      "Every effort moves you , the the a, the, the the the, the, the, the.                                 \n",
      "Ep 12 (Step 000125): Train loss 5.498, Val loss 6.438\n",
      "Ep 12 (Step 000130): Train loss 5.514, Val loss 6.568\n",
      "Every effort moves you , the the a, the the the the the.                                       \n",
      "Ep 13 (Step 000135): Train loss 5.402, Val loss 6.460\n",
      "Ep 13 (Step 000140): Train loss 5.450, Val loss 6.508\n",
      "Every effort moves you , and, a, and, and, and, and, and, I had.                                \n",
      "Ep 14 (Step 000145): Train loss 5.292, Val loss 6.426\n",
      "Ep 14 (Step 000150): Train loss 5.265, Val loss 6.516\n",
      "Every effort moves you , and, the.                                             \n",
      "Ep 15 (Step 000155): Train loss 5.128, Val loss 6.332\n",
      "Ep 15 (Step 000160): Train loss 5.126, Val loss 6.401\n",
      "Every effort moves you , and, the.                                             \n",
      "Ep 16 (Step 000165): Train loss 5.011, Val loss 6.320\n",
      "Ep 16 (Step 000170): Train loss 5.051, Val loss 6.374\n",
      "Ep 16 (Step 000175): Train loss 4.979, Val loss 6.339\n",
      "Every effort moves you , and, and, and, and, and, and, and, I had of the of the.                            \n",
      "Ep 17 (Step 000180): Train loss 4.830, Val loss 6.328\n",
      "Ep 17 (Step 000185): Train loss 4.754, Val loss 6.457\n",
      "Every effort moves you , and, and, and, and, and, and, and, I had to the to the.                            \n",
      "Ep 18 (Step 000190): Train loss 4.767, Val loss 6.255\n",
      "Ep 18 (Step 000195): Train loss 4.701, Val loss 6.243\n",
      "Every effort moves you , and, and, I had to the picture.                                       \n",
      "Ep 19 (Step 000200): Train loss 4.689, Val loss 6.215\n",
      "Ep 19 (Step 000205): Train loss 4.707, Val loss 6.223\n",
      "Every effort moves you , and, and, and, and, and, and, and, I had to the to the picture to the picture.                        \n",
      "Ep 20 (Step 000210): Train loss 4.577, Val loss 6.243\n",
      "Ep 20 (Step 000215): Train loss 4.572, Val loss 6.304\n",
      "Every effort moves you , and, and, and, and, and, and, and, I had to the picture.                             \n",
      "Ep 21 (Step 000220): Train loss 4.440, Val loss 6.260\n",
      "Ep 21 (Step 000225): Train loss 4.451, Val loss 6.180\n",
      "Ep 21 (Step 000230): Train loss 4.280, Val loss 6.155\n",
      "Every effort moves you , and, and, and, and, and, and, and, I had to the picture the picture.    \"I had to the picture.        \"I was, and I had been\n",
      "Ep 22 (Step 000235): Train loss 4.308, Val loss 6.138\n",
      "Ep 22 (Step 000240): Train loss 4.255, Val loss 6.327\n",
      "Every effort moves you , and, and, and, and, and, and, and, I had to the last the picture.                   \"I was, and a little-\n",
      "Ep 23 (Step 000245): Train loss 4.221, Val loss 6.152\n",
      "Ep 23 (Step 000250): Train loss 4.160, Val loss 6.315\n",
      "Every effort moves you , and, and, and it.     \"I was a little of the Riv, and I had to have.      \"Oh, and I had to have had been of the Riv I had been\n",
      "Ep 24 (Step 000255): Train loss 4.059, Val loss 6.343\n",
      "Ep 24 (Step 000260): Train loss 4.025, Val loss 6.331\n",
      "Every effort moves you , and, and, and I had to have, and I had been I had been to the picture to see the picture.                \"Oh, and I had been on\n",
      "Ep 25 (Step 000265): Train loss 3.908, Val loss 6.168\n",
      "Ep 25 (Step 000270): Train loss 3.902, Val loss 6.324\n",
      "Every effort moves you , and, and, and, and, and, and, with a little.   \"Oh, and I had been of the picture was the picture.        \"Oh, and I had been of\n",
      "Ep 26 (Step 000275): Train loss 3.805, Val loss 6.099\n",
      "Ep 26 (Step 000280): Train loss 3.867, Val loss 6.180\n",
      "Ep 26 (Step 000285): Train loss 3.821, Val loss 6.098\n",
      "Every effort moves you , and, and, and I had to the picture.    \"I't of the picture.                   \"Oh.     \n",
      "Ep 27 (Step 000290): Train loss 3.710, Val loss 6.138\n",
      "Ep 27 (Step 000295): Train loss 3.693, Val loss 6.293\n",
      "Every effort moves you , and, and, and and in the picture.  \"I was a little of the picture to see it was the picture.      \"Oh, and in a little of the picture to have said a little-\n",
      "Ep 28 (Step 000300): Train loss 3.650, Val loss 6.148\n",
      "Ep 28 (Step 000305): Train loss 3.643, Val loss 6.148\n",
      "Every effort moves you , and, and, and I had been of the picture.                     \"Oh, and I said, and said, and I had a little-\n",
      "Ep 29 (Step 000310): Train loss 3.490, Val loss 6.068\n",
      "Ep 29 (Step 000315): Train loss 3.477, Val loss 6.289\n",
      "Every effort moves you , and, in the picture, and, and, and, with a little.   \"Oh, and I had been of the picture was the Riv of the picture to see it.  \"Oh, and I had been of\n",
      "Ep 30 (Step 000320): Train loss 3.431, Val loss 6.077\n",
      "Ep 30 (Step 000325): Train loss 3.472, Val loss 6.087\n",
      "Every effort moves you , and, and, and I had been of the Riv.   \"I said to the picture--as, and in the picture to have was a little.       \"Oh, and I had been you\n",
      "Training completed in 9.60 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "model = smalLM_model(smalLM_config)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_dataloader, val_dataloader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you \", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2315e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76640805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
