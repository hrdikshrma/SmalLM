
# SmalLM 🦾

Welcome to **SmalLM**! This repository contains a custom-built Transformer-based language model with approximately 25 million parameters, implemented entirely from scratch using Python and PyTorch (or your chosen framework)

⚠️ **Note:** This is an **ongoing project**. I am actively developing and refining the codebase, which means that parts of it may change frequently — including the addition, removal, or refactoring of modules and features.



## 🛠️ Architecture

 The model architecture closely follows the foundational design of the Transformer, incorporating core components such as:

- Token and positional embeddings  
- Multi-head self-attention  
- Layer normalization  
- Feedforward networks  
- Transformer blocks  
- Causal masking for autoregressive language modeling
## 🚧 Project Status

- ✅ Core transformer architecture implemented  
- ✅ Supports basic training loop 
- 🔄 Currently optimizing performance and adding tests  
- 🔜 Future plans include finetuning and temperature scaling techniques
## 🧠 Model Overview

- Total parameters: ~25 million  
- Architecture: Transformer decoder (GPT-style)  
- Number of layers, heads, and dimensions are configurable  
- Implements causal language modeling objective
## 🤝 Contributing

Contributions are welcome! If you have a bug fix, improvement, or a new feature to add, please follow these steps:
- Fork the repository
- Create a new branch
- Make your changes and commit them
- Push to the branch
- Open a Pull Request
## 📞 Contacts

If you have any questions or suggestions, feel free to reach out to me:
- Email: hrdikshrma.contact@gmail.com
- GitHub: hrdikshrma
Thank you for visiting SmalLM! Happy coding!