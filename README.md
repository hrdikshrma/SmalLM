
# SmalLM ğŸ¦¾

Welcome to **SmalLM**! This repository contains a custom-built Transformer-based language model with approximately 25 million parameters, implemented entirely from scratch using Python and PyTorch (or your chosen framework)

âš ï¸ **Note:** This is an **ongoing project**. I am actively developing and refining the codebase, which means that parts of it may change frequently â€” including the addition, removal, or refactoring of modules and features.



## ğŸ› ï¸ Architecture

 The model architecture closely follows the foundational design of the Transformer, incorporating core components such as:

- Token and positional embeddings  
- Multi-head self-attention  
- Layer normalization  
- Feedforward networks  
- Transformer blocks  
- Causal masking for autoregressive language modeling
## ğŸš§ Project Status

- âœ… Core transformer architecture implemented  
- âœ… Supports basic training loop 
- ğŸ”„ Currently optimizing performance and adding tests  
- ğŸ”œ Future plans include finetuning and temperature scaling techniques
## ğŸ§  Model Overview

- Total parameters: ~25 million  
- Architecture: Transformer decoder (GPT-style)  
- Number of layers, heads, and dimensions are configurable  
- Implements causal language modeling objective
## ğŸ¤ Contributing

Contributions are welcome! If you have a bug fix, improvement, or a new feature to add, please follow these steps:
- Fork the repository
- Create a new branch
- Make your changes and commit them
- Push to the branch
- Open a Pull Request
## ğŸ“ Contacts

If you have any questions or suggestions, feel free to reach out to me:
- Email: hrdikshrma.contact@gmail.com
- GitHub: hrdikshrma
Thank you for visiting SmalLM! Happy coding!